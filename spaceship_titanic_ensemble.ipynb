{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "Z1Ix1OwZUjfI",
        "xmHcrKb1FuiN"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "httXSBFtABTj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa95f1ff-3359-4a70-9422-24797b409935"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7yo-RIS3_jng"
      },
      "outputs": [],
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd \n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Read data"
      ],
      "metadata": {
        "id": "UfAYUfhlDiqA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pathlib\n",
        "data_dir = pathlib.Path('/content/drive/MyDrive/ColabNotebooks/spaceship_titanic')\n",
        "\n",
        "df_train = pd.read_csv(data_dir / \"train.csv\")\n",
        "df_test = pd.read_csv(data_dir / \"test.csv\")\n",
        "df_test_original = pd.read_csv(data_dir / \"test.csv\")\n",
        "\n",
        "#print(df_train.describe())"
      ],
      "metadata": {
        "id": "FmgO876lAPNU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Check Nulls"
      ],
      "metadata": {
        "id": "o2pbUxFlS8_I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "total = df_train.isnull().sum().sort_values(ascending = False)\n",
        "percent = (df_train.isnull().sum()/df_train.isnull().count()*100).sort_values(ascending = False)\n",
        "missing_data  = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n",
        "print(missing_data)"
      ],
      "metadata": {
        "id": "sb7BJ7sCTA8V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c45d3eaf-93a5-4589-d0f0-6062f0189a53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              Total   Percent\n",
            "CryoSleep       217  2.496261\n",
            "ShoppingMall    208  2.392730\n",
            "VIP             203  2.335212\n",
            "HomePlanet      201  2.312205\n",
            "Name            200  2.300702\n",
            "Cabin           199  2.289198\n",
            "VRDeck          188  2.162660\n",
            "FoodCourt       183  2.105142\n",
            "Spa             183  2.105142\n",
            "Destination     182  2.093639\n",
            "RoomService     181  2.082135\n",
            "Age             179  2.059128\n",
            "PassengerId       0  0.000000\n",
            "Transported       0  0.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Add Variables"
      ],
      "metadata": {
        "id": "uiGnN6arT9TV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "col_to_sum = ['RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']\n",
        "\n",
        "#Create variable SumSpends to gather the sum of spent value by each passenger\n",
        "df_train['SumSpends'] = df_train[col_to_sum].sum(axis=1)\n",
        "df_test['SumSpends'] = df_test[col_to_sum].sum(axis=1)\n",
        "\n",
        "\n",
        "#Cabin column - split into 3 columns, deck, num and side\n",
        "df_train[['deck','num', 'side']] = df_train['Cabin'].str.split('/', 3, expand=True)\n",
        "df_test[['deck','num', 'side']] = df_test['Cabin'].str.split('/', 3, expand=True)\n",
        "\n",
        "df_train.drop('Cabin', axis=1, inplace=True)\n",
        "df_test.drop('Cabin', axis=1, inplace=True)\n",
        "\n",
        "#encode categorical variables as integer to improve model performance\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "oc = OrdinalEncoder()\n",
        "\n",
        "size_train = len(df_train)\n",
        "\n",
        "#join train and test datasets for encoding\n",
        "df_for_encode = pd.concat([df_train, df_test])\n",
        "\n",
        "#get columns to be encoded\n",
        "object_cols = [col for col in df_train.columns if df_train[col].dtype == 'object' or df_train[col].dtype == 'category']\n",
        "object_cols.append('Transported')\n",
        "\n",
        "#convert to category to save space\n",
        "df_for_encode[object_cols] = df_for_encode[object_cols].astype('category')\n",
        "#encode values\n",
        "df_for_encode[object_cols] = oc.fit_transform(df_for_encode[object_cols])\n",
        "\n",
        "del df_train, df_test\n",
        "\n",
        "#split train and test datasets\n",
        "df_train = df_for_encode.iloc[:size_train, :]\n",
        "df_test = df_for_encode.iloc[size_train: , :]\n",
        "df_test.drop('Transported', axis=1, inplace=True)\n",
        "\n",
        "del df_for_encode"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nLvaKjGIUAmS",
        "outputId": "9b01664a-de61-49f3-be96-e3c7b5c088fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py:4913: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  errors=errors,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Data Cleaning"
      ],
      "metadata": {
        "id": "_GMTHP18ZsoN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "object_cols = [col for col in df_train.columns if df_train[col].dtype == 'object' or df_train[col].dtype == 'category']\n",
        "\n",
        "df_train[object_cols] = df_train[object_cols].astype('category')\n",
        "df_test[object_cols] = df_test[object_cols].astype('category')\n",
        "\n",
        "null_cols = df_train.isnull().sum().sort_values(ascending=False)\n",
        "null_cols = list(null_cols[null_cols>1].index)\n",
        "\n",
        "\n",
        "#Replace null values using sklearn SimpleImputer, different strategies are used for different columns\n",
        "toMedian = ['num']\n",
        "toMode = ['CryoSleep','VIP','HomePlanet','Name','deck','side']\n",
        "toMean = ['Destination','Age','SumSpends','RoomService','FoodCourt','ShoppingMall','Spa','VRDeck']\n",
        "\n",
        "ct = ColumnTransformer([(\"imp\", SimpleImputer(strategy='mean'), toMean)])\n",
        "df_train[toMean] = ct.fit_transform(df_train[toMean])\n",
        "df_test[toMean] = ct.fit_transform(df_test[toMean])\n",
        "\n",
        "\n",
        "ct2 = ColumnTransformer([(\"imp\", SimpleImputer(strategy='median'), toMedian)])\n",
        "df_train[toMedian] = ct2.fit_transform(df_train[toMedian])\n",
        "df_test[toMedian] = ct2.fit_transform(df_test[toMedian])\n",
        "\n",
        "\n",
        "ct3 = ColumnTransformer([(\"imp\", SimpleImputer(strategy='most_frequent'), toMode)])\n",
        "df_train[toMode] = ct3.fit_transform(df_train[toMode])\n",
        "df_test[toMode] = ct3.fit_transform(df_test[toMode])\n",
        "\n"
      ],
      "metadata": {
        "id": "BHFlRsk-ZxOb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.head(35)"
      ],
      "metadata": {
        "id": "GLCSAE5DjuUD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "06ca2dba-a8ad-44dd-c44d-cfa9ddbaa03d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    PassengerId  HomePlanet  CryoSleep  Destination   Age  VIP  RoomService  \\\n",
              "0           0.0         1.0        0.0          2.0  39.0  0.0     0.000000   \n",
              "1           1.0         0.0        0.0          2.0  24.0  0.0   109.000000   \n",
              "2           2.0         1.0        0.0          2.0  58.0  1.0    43.000000   \n",
              "3           3.0         1.0        0.0          2.0  33.0  0.0     0.000000   \n",
              "4           4.0         0.0        0.0          2.0  16.0  0.0   303.000000   \n",
              "5           5.0         0.0        0.0          1.0  44.0  0.0     0.000000   \n",
              "6           6.0         0.0        0.0          2.0  26.0  0.0    42.000000   \n",
              "7           7.0         0.0        1.0          2.0  28.0  0.0     0.000000   \n",
              "8           8.0         0.0        0.0          2.0  35.0  0.0     0.000000   \n",
              "9           9.0         1.0        1.0          0.0  14.0  0.0     0.000000   \n",
              "10         10.0         1.0        1.0          2.0  34.0  0.0     0.000000   \n",
              "11         11.0         1.0        0.0          0.0  45.0  0.0    39.000000   \n",
              "12         12.0         2.0        0.0          2.0  32.0  0.0    73.000000   \n",
              "13         13.0         0.0        0.0          2.0  48.0  0.0   719.000000   \n",
              "14         14.0         0.0        0.0          2.0  28.0  0.0     8.000000   \n",
              "15         15.0         0.0        0.0          2.0  31.0  0.0    32.000000   \n",
              "16         17.0         2.0        0.0          0.0  27.0  0.0  1286.000000   \n",
              "17         18.0         0.0        0.0          0.0  24.0  0.0     0.000000   \n",
              "18         19.0         2.0        1.0          2.0  45.0  0.0     0.000000   \n",
              "19         20.0         0.0        0.0          2.0   0.0  0.0     0.000000   \n",
              "20         21.0         0.0        0.0          0.0  14.0  0.0   412.000000   \n",
              "21         24.0         0.0        1.0          2.0   1.0  0.0     0.000000   \n",
              "22         25.0         0.0        1.0          0.0  49.0  0.0     0.000000   \n",
              "23         26.0         0.0        1.0          0.0  29.0  0.0     0.000000   \n",
              "24         27.0         0.0        0.0          2.0  10.0  0.0     0.000000   \n",
              "25         28.0         0.0        1.0          1.0   1.0  0.0   224.687617   \n",
              "26         29.0         0.0        0.0          2.0   7.0  0.0     0.000000   \n",
              "27         31.0         2.0        0.0          2.0  21.0  0.0   980.000000   \n",
              "28         33.0         1.0        1.0          2.0  62.0  0.0     0.000000   \n",
              "29         34.0         0.0        0.0          2.0  15.0  0.0     0.000000   \n",
              "30         35.0         1.0        0.0          0.0  34.0  0.0    22.000000   \n",
              "31         37.0         2.0        0.0          2.0  43.0  0.0  1125.000000   \n",
              "32         39.0         0.0        0.0          2.0  32.0  0.0     0.000000   \n",
              "33         40.0         2.0        0.0          2.0  47.0  0.0   214.000000   \n",
              "34         41.0         2.0        0.0          2.0   2.0  0.0     0.000000   \n",
              "\n",
              "    FoodCourt  ShoppingMall     Spa       VRDeck     Name  Transported  \\\n",
              "0         0.0      0.000000     0.0     0.000000   7819.0          0.0   \n",
              "1         9.0     25.000000   549.0    44.000000   6688.0          1.0   \n",
              "2      3576.0      0.000000  6715.0    49.000000    669.0          0.0   \n",
              "3      1283.0    371.000000  3329.0   193.000000  10688.0          0.0   \n",
              "4        70.0    151.000000   565.0     2.000000  12400.0          1.0   \n",
              "5       483.0      0.000000   291.0     0.000000  10283.0          1.0   \n",
              "6      1539.0      3.000000     0.0     0.000000   1733.0          1.0   \n",
              "7         0.0      0.000000     0.0   304.854791   2182.0          1.0   \n",
              "8       785.0     17.000000   216.0     0.000000    842.0          1.0   \n",
              "9         0.0      0.000000     0.0     0.000000   4286.0          1.0   \n",
              "10        0.0    173.729169     0.0     0.000000    663.0          1.0   \n",
              "11     7295.0    589.000000   110.0   124.000000  12314.0          1.0   \n",
              "12        0.0   1123.000000     0.0   113.000000   1623.0          1.0   \n",
              "13        1.0     65.000000     0.0    24.000000   9830.0          0.0   \n",
              "14      974.0     12.000000     2.0     7.000000   4051.0          1.0   \n",
              "15        0.0    876.000000     0.0     0.000000   6742.0          0.0   \n",
              "16      122.0    173.729169     0.0     0.000000   4580.0          0.0   \n",
              "17        1.0      0.000000     0.0   637.000000   2314.0          0.0   \n",
              "18        0.0      0.000000     0.0     0.000000    685.0          1.0   \n",
              "19        0.0      0.000000     0.0     0.000000   7700.0          1.0   \n",
              "20        0.0      1.000000     0.0   679.000000   9304.0          0.0   \n",
              "21        0.0      0.000000     0.0     0.000000    535.0          0.0   \n",
              "22        0.0      0.000000     0.0     0.000000   5025.0          0.0   \n",
              "23        0.0    173.729169     0.0     0.000000   8464.0          0.0   \n",
              "24        0.0      0.000000     0.0     0.000000   2013.0          1.0   \n",
              "25        0.0      0.000000     0.0     0.000000   7795.0          0.0   \n",
              "26        0.0      0.000000     0.0     0.000000  11347.0          0.0   \n",
              "27        2.0     69.000000     0.0     0.000000    605.0          0.0   \n",
              "28        0.0    173.729169     0.0     0.000000   9240.0          1.0   \n",
              "29      225.0      0.000000   998.0     0.000000   6782.0          0.0   \n",
              "30     6073.0      0.000000  1438.0   328.000000    998.0          0.0   \n",
              "31        0.0    136.000000    48.0     0.000000   2436.0          0.0   \n",
              "32      850.0     81.000000   437.0   453.000000   5003.0          0.0   \n",
              "33        0.0   1411.000000     0.0  1229.000000   2846.0          1.0   \n",
              "34        0.0      0.000000     0.0     0.000000   2667.0          1.0   \n",
              "\n",
              "    SumSpends  deck     num  side  \n",
              "0         0.0   1.0     0.0   0.0  \n",
              "1       736.0   5.0     0.0   1.0  \n",
              "2     10383.0   0.0     0.0   1.0  \n",
              "3      5176.0   0.0     0.0   1.0  \n",
              "4      1091.0   5.0     1.0   1.0  \n",
              "5       774.0   5.0     0.0   0.0  \n",
              "6      1584.0   5.0  1006.0   1.0  \n",
              "7         0.0   6.0     0.0   1.0  \n",
              "8      1018.0   5.0  1117.0   1.0  \n",
              "9         0.0   1.0     1.0   0.0  \n",
              "10        0.0   1.0     1.0   0.0  \n",
              "11     8157.0   1.0     1.0   0.0  \n",
              "12     1309.0   5.0     1.0   0.0  \n",
              "13      809.0   6.0     1.0   1.0  \n",
              "14     1003.0   5.0  1006.0   0.0  \n",
              "15      908.0   5.0  1075.0   1.0  \n",
              "16     1408.0   5.0  1117.0   0.0  \n",
              "17      638.0   5.0  1228.0   0.0  \n",
              "18        0.0   5.0  1339.0   0.0  \n",
              "19        0.0   6.0     0.0   0.0  \n",
              "20     1092.0   5.0  1450.0   0.0  \n",
              "21        0.0   4.0     0.0   1.0  \n",
              "22        0.0   4.0     0.0   1.0  \n",
              "23        0.0   4.0     0.0   1.0  \n",
              "24        0.0   4.0     0.0   1.0  \n",
              "25        0.0   4.0     0.0   1.0  \n",
              "26        0.0   4.0     0.0   1.0  \n",
              "27     1051.0   3.0     0.0   0.0  \n",
              "28        0.0   2.0  1006.0   1.0  \n",
              "29     1223.0   5.0  1450.0   1.0  \n",
              "30     7861.0   2.0     0.0   0.0  \n",
              "31     1309.0   5.0  1672.0   0.0  \n",
              "32     1821.0   6.0  1228.0   1.0  \n",
              "33     2854.0   5.0  1783.0   0.0  \n",
              "34        0.0   5.0  1783.0   0.0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e04671b5-5ad3-4278-a897-b1bfa0115a1f\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PassengerId</th>\n",
              "      <th>HomePlanet</th>\n",
              "      <th>CryoSleep</th>\n",
              "      <th>Destination</th>\n",
              "      <th>Age</th>\n",
              "      <th>VIP</th>\n",
              "      <th>RoomService</th>\n",
              "      <th>FoodCourt</th>\n",
              "      <th>ShoppingMall</th>\n",
              "      <th>Spa</th>\n",
              "      <th>VRDeck</th>\n",
              "      <th>Name</th>\n",
              "      <th>Transported</th>\n",
              "      <th>SumSpends</th>\n",
              "      <th>deck</th>\n",
              "      <th>num</th>\n",
              "      <th>side</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>39.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>7819.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>24.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>109.000000</td>\n",
              "      <td>9.0</td>\n",
              "      <td>25.000000</td>\n",
              "      <td>549.0</td>\n",
              "      <td>44.000000</td>\n",
              "      <td>6688.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>736.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>58.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>43.000000</td>\n",
              "      <td>3576.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>6715.0</td>\n",
              "      <td>49.000000</td>\n",
              "      <td>669.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>10383.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>33.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1283.0</td>\n",
              "      <td>371.000000</td>\n",
              "      <td>3329.0</td>\n",
              "      <td>193.000000</td>\n",
              "      <td>10688.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5176.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>303.000000</td>\n",
              "      <td>70.0</td>\n",
              "      <td>151.000000</td>\n",
              "      <td>565.0</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>12400.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1091.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>44.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>483.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>291.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>10283.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>774.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>6.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>26.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>42.000000</td>\n",
              "      <td>1539.0</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1733.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1584.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1006.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>7.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>28.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>304.854791</td>\n",
              "      <td>2182.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>8.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>35.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>785.0</td>\n",
              "      <td>17.000000</td>\n",
              "      <td>216.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>842.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1018.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1117.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>9.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4286.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>10.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>34.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>173.729169</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>663.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>11.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>39.000000</td>\n",
              "      <td>7295.0</td>\n",
              "      <td>589.000000</td>\n",
              "      <td>110.0</td>\n",
              "      <td>124.000000</td>\n",
              "      <td>12314.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>8157.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>12.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>32.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>73.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1123.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>113.000000</td>\n",
              "      <td>1623.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1309.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>13.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>48.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>719.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>65.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>9830.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>809.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>14.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>28.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>974.0</td>\n",
              "      <td>12.000000</td>\n",
              "      <td>2.0</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>4051.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1003.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1006.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>15.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>31.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>32.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>876.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>6742.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>908.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1075.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>17.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>27.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1286.000000</td>\n",
              "      <td>122.0</td>\n",
              "      <td>173.729169</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4580.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1408.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1117.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>18.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>24.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>637.000000</td>\n",
              "      <td>2314.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>638.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1228.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>19.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>685.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1339.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>20.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>7700.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>21.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>412.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>679.000000</td>\n",
              "      <td>9304.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1092.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1450.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>24.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>535.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>25.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>49.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>5025.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>26.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>29.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>173.729169</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>8464.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>27.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2013.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>28.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>224.687617</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>7795.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>29.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>11347.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>31.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>980.000000</td>\n",
              "      <td>2.0</td>\n",
              "      <td>69.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>605.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1051.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>33.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>62.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>173.729169</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>9240.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1006.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>34.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>225.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>998.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>6782.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1223.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1450.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>35.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>34.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>22.000000</td>\n",
              "      <td>6073.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1438.0</td>\n",
              "      <td>328.000000</td>\n",
              "      <td>998.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7861.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>37.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>43.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1125.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>136.000000</td>\n",
              "      <td>48.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2436.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1309.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1672.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>39.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>32.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>850.0</td>\n",
              "      <td>81.000000</td>\n",
              "      <td>437.0</td>\n",
              "      <td>453.000000</td>\n",
              "      <td>5003.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1821.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>1228.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>40.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>47.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>214.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1411.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1229.000000</td>\n",
              "      <td>2846.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2854.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1783.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>41.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2667.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1783.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e04671b5-5ad3-4278-a897-b1bfa0115a1f')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-e04671b5-5ad3-4278-a897-b1bfa0115a1f button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-e04671b5-5ad3-4278-a897-b1bfa0115a1f');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Delete Outliers"
      ],
      "metadata": {
        "id": "Z1Ix1OwZUjfI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns \n",
        "from matplotlib.pyplot import figure\n",
        "\n",
        "clean_outliers = False\n",
        "print_boxplot = False\n",
        "\n",
        "cols_with_outliers = ['RoomService','FoodCourt','ShoppingMall','Spa','VRDeck']\n",
        "threshold_outliers = [10000, 25000, 10000, 15000, 18000]\n",
        "\n",
        "if clean_outliers:\n",
        "  for index, var in enumerate(cols_with_outliers):\n",
        "    print(df_train[var].shape)\n",
        "    df_train.drop(df_train[df_train[var] > threshold_outliers[index]].index, axis=0, inplace=True)\n",
        "    print(df_train[var].shape)\n",
        "\n",
        "if print_boxplot:\n",
        "  for index, var in enumerate(df_train.columns):\n",
        "    figure(figsize=(8, 6), dpi=80)\n",
        "    sns.boxplot(data=df_train[var], orient='h')\n",
        "    plt.xlabel(var)\n",
        "    plt.xlim((0,25000))\n",
        "    plt.show()\n",
        "    plt.clf()"
      ],
      "metadata": {
        "id": "JHE_qhSrUjfN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Log Transform"
      ],
      "metadata": {
        "id": "xmHcrKb1FuiN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "log_transform = False\n",
        "\n",
        "if log_transform:\n",
        "  import seaborn as sns\n",
        "  cols_logs = ['FoodCourt','ShoppingMall','Spa','VRDeck']\n",
        "  #teste=df_train.copy(deep=True)\n",
        "\n",
        "  df_train[cols_logs] = df_train[cols_logs].replace(0, 0.0001)\n",
        "\n",
        "  for col in cols_logs:\n",
        "    df_train[col] = np.log(df_train[col])\n",
        "    #sns.histplot(df_train[col])\n",
        "    #plt.show()\n",
        "    #plt.clf()\n"
      ],
      "metadata": {
        "id": "63TtuMGfF6X1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "LhwM2er5GJkV",
        "outputId": "b61b8e5f-4817-4f0f-b253-c96f4063a83c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      PassengerId  HomePlanet  CryoSleep  Destination   Age  VIP  RoomService  \\\n",
              "0             0.0         1.0        0.0          2.0  39.0  0.0          0.0   \n",
              "1             1.0         0.0        0.0          2.0  24.0  0.0        109.0   \n",
              "2             2.0         1.0        0.0          2.0  58.0  1.0         43.0   \n",
              "3             3.0         1.0        0.0          2.0  33.0  0.0          0.0   \n",
              "4             4.0         0.0        0.0          2.0  16.0  0.0        303.0   \n",
              "...           ...         ...        ...          ...   ...  ...          ...   \n",
              "8688      12964.0         1.0        0.0          0.0  41.0  1.0          0.0   \n",
              "8689      12966.0         0.0        1.0          1.0  18.0  0.0          0.0   \n",
              "8690      12967.0         0.0        0.0          2.0  26.0  0.0          0.0   \n",
              "8691      12968.0         1.0        0.0          0.0  32.0  0.0          0.0   \n",
              "8692      12969.0         1.0        0.0          2.0  44.0  0.0        126.0   \n",
              "\n",
              "      FoodCourt  ShoppingMall     Spa  VRDeck     Name  Transported  \\\n",
              "0           0.0           0.0     0.0     0.0   7819.0          0.0   \n",
              "1           9.0          25.0   549.0    44.0   6688.0          1.0   \n",
              "2        3576.0           0.0  6715.0    49.0    669.0          0.0   \n",
              "3        1283.0         371.0  3329.0   193.0  10688.0          0.0   \n",
              "4          70.0         151.0   565.0     2.0  12400.0          1.0   \n",
              "...         ...           ...     ...     ...      ...          ...   \n",
              "8688     6819.0           0.0  1643.0    74.0   5252.0          0.0   \n",
              "8689        0.0           0.0     0.0     0.0   7124.0          0.0   \n",
              "8690        0.0        1872.0     1.0     0.0   4498.0          1.0   \n",
              "8691     1049.0           0.0   353.0  3235.0   2389.0          0.0   \n",
              "8692     4688.0           0.0     0.0    12.0   9502.0          1.0   \n",
              "\n",
              "      SumSpends  deck     num  side  \n",
              "0           0.0   1.0     0.0   0.0  \n",
              "1         736.0   5.0     0.0   1.0  \n",
              "2       10383.0   0.0     0.0   1.0  \n",
              "3        5176.0   0.0     0.0   1.0  \n",
              "4        1091.0   5.0     1.0   1.0  \n",
              "...         ...   ...     ...   ...  \n",
              "8688     8536.0   0.0  1872.0   0.0  \n",
              "8689        0.0   6.0   556.0   1.0  \n",
              "8690     1873.0   6.0   559.0   1.0  \n",
              "8691     4637.0   4.0  1460.0   1.0  \n",
              "8692     4826.0   4.0  1460.0   1.0  \n",
              "\n",
              "[8693 rows x 17 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-5c5a6b8c-5677-46ea-9017-680a4bde9951\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PassengerId</th>\n",
              "      <th>HomePlanet</th>\n",
              "      <th>CryoSleep</th>\n",
              "      <th>Destination</th>\n",
              "      <th>Age</th>\n",
              "      <th>VIP</th>\n",
              "      <th>RoomService</th>\n",
              "      <th>FoodCourt</th>\n",
              "      <th>ShoppingMall</th>\n",
              "      <th>Spa</th>\n",
              "      <th>VRDeck</th>\n",
              "      <th>Name</th>\n",
              "      <th>Transported</th>\n",
              "      <th>SumSpends</th>\n",
              "      <th>deck</th>\n",
              "      <th>num</th>\n",
              "      <th>side</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>39.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7819.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>24.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>109.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>549.0</td>\n",
              "      <td>44.0</td>\n",
              "      <td>6688.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>736.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>58.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>43.0</td>\n",
              "      <td>3576.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6715.0</td>\n",
              "      <td>49.0</td>\n",
              "      <td>669.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>10383.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>33.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1283.0</td>\n",
              "      <td>371.0</td>\n",
              "      <td>3329.0</td>\n",
              "      <td>193.0</td>\n",
              "      <td>10688.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5176.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>303.0</td>\n",
              "      <td>70.0</td>\n",
              "      <td>151.0</td>\n",
              "      <td>565.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>12400.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1091.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8688</th>\n",
              "      <td>12964.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>41.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6819.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1643.0</td>\n",
              "      <td>74.0</td>\n",
              "      <td>5252.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8536.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1872.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8689</th>\n",
              "      <td>12966.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>18.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7124.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>556.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8690</th>\n",
              "      <td>12967.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>26.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1872.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4498.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1873.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>559.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8691</th>\n",
              "      <td>12968.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>32.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1049.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>353.0</td>\n",
              "      <td>3235.0</td>\n",
              "      <td>2389.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4637.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1460.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8692</th>\n",
              "      <td>12969.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>44.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>126.0</td>\n",
              "      <td>4688.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>9502.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4826.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1460.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8693 rows  17 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5c5a6b8c-5677-46ea-9017-680a4bde9951')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-5c5a6b8c-5677-46ea-9017-680a4bde9951 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-5c5a6b8c-5677-46ea-9017-680a4bde9951');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Normalization\n",
        "\n"
      ],
      "metadata": {
        "id": "OmeMrAxFNE7G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()#z-score normalization\n",
        "\n",
        "df_train2 = df_train.copy(deep=True)#deep = True to create new copy\n",
        "df_test2 = df_test.copy(deep=True)\n",
        "\n",
        "#vars_to_normalize = ['RoomService', 'Spa', 'VRDeck', 'SumSpends']\n",
        "vars_to_normalize = df_test2.columns.to_list()\n",
        "vars_to_normalize.remove('CryoSleep')\n",
        "print(vars_to_normalize)\n",
        "\n",
        "scaler.fit(df_train2[vars_to_normalize])\n",
        "df_train2[vars_to_normalize] = scaler.transform(df_train2[vars_to_normalize])\n",
        "df_test2[vars_to_normalize] = scaler.transform(df_test2[vars_to_normalize])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P3DEmsnKeg-u",
        "outputId": "af7bf76c-6cda-44d6-dd44-ce0f986c4959"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['PassengerId', 'HomePlanet', 'Destination', 'Age', 'VIP', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck', 'Name', 'SumSpends', 'deck', 'num', 'side']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df_train2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "FktsqsI_etmi",
        "outputId": "202b9583-a23b-498b-d607-0bcc75a2e705"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      PassengerId  HomePlanet  CryoSleep  Destination       Age       VIP  \\\n",
              "0       -1.740838    0.440385        0.0     0.636441  0.709437 -0.153063   \n",
              "1       -1.740569   -0.817259        0.0     0.636441 -0.336717 -0.153063   \n",
              "2       -1.740301    0.440385        0.0     0.636441  2.034566  6.533255   \n",
              "3       -1.740032    0.440385        0.0     0.636441  0.290975 -0.153063   \n",
              "4       -1.739764   -0.817259        0.0     0.636441 -0.894666 -0.153063   \n",
              "...           ...         ...        ...          ...       ...       ...   \n",
              "8688     1.741038    0.440385        0.0    -1.827957  0.848924  6.533255   \n",
              "8689     1.741575   -0.817259        1.0    -0.595758 -0.755179 -0.153063   \n",
              "8690     1.741844   -0.817259        0.0     0.636441 -0.197230 -0.153063   \n",
              "8691     1.742112    0.440385        0.0    -1.827957  0.221232 -0.153063   \n",
              "8692     1.742381    0.440385        0.0     0.636441  1.058155 -0.153063   \n",
              "\n",
              "      RoomService  FoodCourt  ShoppingMall       Spa    VRDeck      Name  \\\n",
              "0       -0.340590  -0.287314     -0.290817 -0.276663 -0.269023  0.440548   \n",
              "1       -0.175364  -0.281669     -0.248968  0.211505 -0.230194  0.135368   \n",
              "2       -0.275409   1.955616     -0.290817  5.694289 -0.225782 -1.488746   \n",
              "3       -0.340590   0.517406      0.330225  2.683471 -0.098708  1.214694   \n",
              "4        0.118709  -0.243409     -0.038048  0.225732 -0.267258  1.676645   \n",
              "...           ...        ...           ...       ...       ...       ...   \n",
              "8688    -0.340590   3.989682     -0.290817  1.184286 -0.203720 -0.252109   \n",
              "8689    -0.340590  -0.287314     -0.290817 -0.276663 -0.269023  0.253015   \n",
              "8690    -0.340590  -0.287314      2.842851 -0.275774 -0.269023 -0.455562   \n",
              "8691    -0.340590   0.370637     -0.290817  0.037223  2.585740 -1.024637   \n",
              "8692    -0.149594   2.653082     -0.290817 -0.276663 -0.258433  0.894674   \n",
              "\n",
              "      Transported  SumSpends      deck       num      side  \n",
              "0             0.0  -0.514066 -1.886321 -1.895131 -1.032865  \n",
              "1             1.0  -0.251479  0.385470 -1.895131  0.968181  \n",
              "2             0.0   3.190333 -2.454269 -1.895131  0.968181  \n",
              "3             0.0   1.332604 -2.454269 -1.895131  0.968181  \n",
              "4             1.0  -0.124824  0.385470 -1.893235  0.968181  \n",
              "...           ...        ...       ...       ...       ...  \n",
              "8688          0.0   2.531369 -2.454269  1.653523 -1.032865  \n",
              "8689          0.0  -0.514066  0.953418 -0.841150  0.968181  \n",
              "8690          1.0   0.154175  0.953418 -0.835463  0.968181  \n",
              "8691          0.0   1.140302 -0.182478  0.872516  0.968181  \n",
              "8692          1.0   1.207732 -0.182478  0.872516  0.968181  \n",
              "\n",
              "[8693 rows x 17 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-655d4259-0960-45e9-aa1a-ea6811abe30c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PassengerId</th>\n",
              "      <th>HomePlanet</th>\n",
              "      <th>CryoSleep</th>\n",
              "      <th>Destination</th>\n",
              "      <th>Age</th>\n",
              "      <th>VIP</th>\n",
              "      <th>RoomService</th>\n",
              "      <th>FoodCourt</th>\n",
              "      <th>ShoppingMall</th>\n",
              "      <th>Spa</th>\n",
              "      <th>VRDeck</th>\n",
              "      <th>Name</th>\n",
              "      <th>Transported</th>\n",
              "      <th>SumSpends</th>\n",
              "      <th>deck</th>\n",
              "      <th>num</th>\n",
              "      <th>side</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-1.740838</td>\n",
              "      <td>0.440385</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.636441</td>\n",
              "      <td>0.709437</td>\n",
              "      <td>-0.153063</td>\n",
              "      <td>-0.340590</td>\n",
              "      <td>-0.287314</td>\n",
              "      <td>-0.290817</td>\n",
              "      <td>-0.276663</td>\n",
              "      <td>-0.269023</td>\n",
              "      <td>0.440548</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.514066</td>\n",
              "      <td>-1.886321</td>\n",
              "      <td>-1.895131</td>\n",
              "      <td>-1.032865</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-1.740569</td>\n",
              "      <td>-0.817259</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.636441</td>\n",
              "      <td>-0.336717</td>\n",
              "      <td>-0.153063</td>\n",
              "      <td>-0.175364</td>\n",
              "      <td>-0.281669</td>\n",
              "      <td>-0.248968</td>\n",
              "      <td>0.211505</td>\n",
              "      <td>-0.230194</td>\n",
              "      <td>0.135368</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-0.251479</td>\n",
              "      <td>0.385470</td>\n",
              "      <td>-1.895131</td>\n",
              "      <td>0.968181</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-1.740301</td>\n",
              "      <td>0.440385</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.636441</td>\n",
              "      <td>2.034566</td>\n",
              "      <td>6.533255</td>\n",
              "      <td>-0.275409</td>\n",
              "      <td>1.955616</td>\n",
              "      <td>-0.290817</td>\n",
              "      <td>5.694289</td>\n",
              "      <td>-0.225782</td>\n",
              "      <td>-1.488746</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.190333</td>\n",
              "      <td>-2.454269</td>\n",
              "      <td>-1.895131</td>\n",
              "      <td>0.968181</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-1.740032</td>\n",
              "      <td>0.440385</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.636441</td>\n",
              "      <td>0.290975</td>\n",
              "      <td>-0.153063</td>\n",
              "      <td>-0.340590</td>\n",
              "      <td>0.517406</td>\n",
              "      <td>0.330225</td>\n",
              "      <td>2.683471</td>\n",
              "      <td>-0.098708</td>\n",
              "      <td>1.214694</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.332604</td>\n",
              "      <td>-2.454269</td>\n",
              "      <td>-1.895131</td>\n",
              "      <td>0.968181</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-1.739764</td>\n",
              "      <td>-0.817259</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.636441</td>\n",
              "      <td>-0.894666</td>\n",
              "      <td>-0.153063</td>\n",
              "      <td>0.118709</td>\n",
              "      <td>-0.243409</td>\n",
              "      <td>-0.038048</td>\n",
              "      <td>0.225732</td>\n",
              "      <td>-0.267258</td>\n",
              "      <td>1.676645</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-0.124824</td>\n",
              "      <td>0.385470</td>\n",
              "      <td>-1.893235</td>\n",
              "      <td>0.968181</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8688</th>\n",
              "      <td>1.741038</td>\n",
              "      <td>0.440385</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-1.827957</td>\n",
              "      <td>0.848924</td>\n",
              "      <td>6.533255</td>\n",
              "      <td>-0.340590</td>\n",
              "      <td>3.989682</td>\n",
              "      <td>-0.290817</td>\n",
              "      <td>1.184286</td>\n",
              "      <td>-0.203720</td>\n",
              "      <td>-0.252109</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.531369</td>\n",
              "      <td>-2.454269</td>\n",
              "      <td>1.653523</td>\n",
              "      <td>-1.032865</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8689</th>\n",
              "      <td>1.741575</td>\n",
              "      <td>-0.817259</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-0.595758</td>\n",
              "      <td>-0.755179</td>\n",
              "      <td>-0.153063</td>\n",
              "      <td>-0.340590</td>\n",
              "      <td>-0.287314</td>\n",
              "      <td>-0.290817</td>\n",
              "      <td>-0.276663</td>\n",
              "      <td>-0.269023</td>\n",
              "      <td>0.253015</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.514066</td>\n",
              "      <td>0.953418</td>\n",
              "      <td>-0.841150</td>\n",
              "      <td>0.968181</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8690</th>\n",
              "      <td>1.741844</td>\n",
              "      <td>-0.817259</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.636441</td>\n",
              "      <td>-0.197230</td>\n",
              "      <td>-0.153063</td>\n",
              "      <td>-0.340590</td>\n",
              "      <td>-0.287314</td>\n",
              "      <td>2.842851</td>\n",
              "      <td>-0.275774</td>\n",
              "      <td>-0.269023</td>\n",
              "      <td>-0.455562</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.154175</td>\n",
              "      <td>0.953418</td>\n",
              "      <td>-0.835463</td>\n",
              "      <td>0.968181</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8691</th>\n",
              "      <td>1.742112</td>\n",
              "      <td>0.440385</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-1.827957</td>\n",
              "      <td>0.221232</td>\n",
              "      <td>-0.153063</td>\n",
              "      <td>-0.340590</td>\n",
              "      <td>0.370637</td>\n",
              "      <td>-0.290817</td>\n",
              "      <td>0.037223</td>\n",
              "      <td>2.585740</td>\n",
              "      <td>-1.024637</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.140302</td>\n",
              "      <td>-0.182478</td>\n",
              "      <td>0.872516</td>\n",
              "      <td>0.968181</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8692</th>\n",
              "      <td>1.742381</td>\n",
              "      <td>0.440385</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.636441</td>\n",
              "      <td>1.058155</td>\n",
              "      <td>-0.153063</td>\n",
              "      <td>-0.149594</td>\n",
              "      <td>2.653082</td>\n",
              "      <td>-0.290817</td>\n",
              "      <td>-0.276663</td>\n",
              "      <td>-0.258433</td>\n",
              "      <td>0.894674</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.207732</td>\n",
              "      <td>-0.182478</td>\n",
              "      <td>0.872516</td>\n",
              "      <td>0.968181</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8693 rows  17 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-655d4259-0960-45e9-aa1a-ea6811abe30c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-655d4259-0960-45e9-aa1a-ea6811abe30c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-655d4259-0960-45e9-aa1a-ea6811abe30c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_train2.isnull().sum()"
      ],
      "metadata": {
        "id": "--uqRzuViZVL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f19ecd9-4b80-4e69-b0a1-0b32326fcb0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PassengerId     0\n",
              "HomePlanet      0\n",
              "CryoSleep       0\n",
              "Destination     0\n",
              "Age             0\n",
              "VIP             0\n",
              "RoomService     0\n",
              "FoodCourt       0\n",
              "ShoppingMall    0\n",
              "Spa             0\n",
              "VRDeck          0\n",
              "Name            0\n",
              "Transported     0\n",
              "SumSpends       0\n",
              "deck            0\n",
              "num             0\n",
              "side            0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "class MyCallback(tf.keras.callbacks.Callback):\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        if logs.get('val_accuracy') is not None and logs.get('val_accuracy') > 0.81:\n",
        "            print(\"Reached 80.8% cal_accuracy so cancelling training!\")\n",
        "            self.model.stop_training = True\n",
        "\n",
        "\n",
        "callbacks = MyCallback()"
      ],
      "metadata": {
        "id": "dudYDFhegRwI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Tensorflow - Training"
      ],
      "metadata": {
        "id": "qORF6coiGzk5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#vars_to_use_training = ['FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck', 'SumSpends','deck','num','HomePlanet', 'Transported']\n",
        "vars_to_use_training = ['CryoSleep', 'RoomService', 'Spa', 'VRDeck', 'SumSpends','deck','side','Transported']\n",
        "df_train2 = df_train2[vars_to_use_training]\n",
        "\n",
        "#train test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(df_train2.drop(['Transported'], axis=1), df_train2['Transported'],\n",
        "                                                    test_size=0.30, random_state=45)"
      ],
      "metadata": {
        "id": "x7GCdZvZhy1p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model = tf.keras.Sequential()\n",
        "#First Hidden Layer\n",
        "dim = len(df_train2.columns) - 1\n",
        "model.add(tf.keras.layers.Dense(256, activation='relu', input_dim=dim))\n",
        "model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
        "\n",
        "#Second  Hidden Layer\n",
        "model.add(tf.keras.layers.Dense(4, activation='sigmoid'))\n",
        "#model.add(tf.keras.layers.Dropout(0.2))\n",
        "\n",
        "#Output Layer\n",
        "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer ='adam',loss='binary_crossentropy', metrics =['accuracy'])\n",
        "\n",
        "model.fit(X_train, y_train, steps_per_epoch = 80, epochs=25, validation_data=(X_test, y_test), callbacks=[callbacks])\n"
      ],
      "metadata": {
        "id": "WzDU0lKYLOr8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f776e80-5b07-448d-b425-bfcec984e01e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "80/80 [==============================] - 2s 8ms/step - loss: 0.5739 - accuracy: 0.7154 - val_loss: 0.4944 - val_accuracy: 0.8048\n",
            "Epoch 2/25\n",
            "80/80 [==============================] - 0s 5ms/step - loss: 0.4991 - accuracy: 0.7893 - val_loss: 0.4815 - val_accuracy: 0.7975\n",
            "Epoch 3/25\n",
            "80/80 [==============================] - 0s 5ms/step - loss: 0.4903 - accuracy: 0.7916 - val_loss: 0.4699 - val_accuracy: 0.8067\n",
            "Epoch 4/25\n",
            "80/80 [==============================] - 0s 5ms/step - loss: 0.4828 - accuracy: 0.7949 - val_loss: 0.4704 - val_accuracy: 0.8018\n",
            "Epoch 5/25\n",
            "80/80 [==============================] - 0s 6ms/step - loss: 0.4795 - accuracy: 0.7916 - val_loss: 0.4637 - val_accuracy: 0.8021\n",
            "Epoch 6/25\n",
            "80/80 [==============================] - 0s 6ms/step - loss: 0.4760 - accuracy: 0.7928 - val_loss: 0.4643 - val_accuracy: 0.7964\n",
            "Epoch 7/25\n",
            "80/80 [==============================] - 0s 5ms/step - loss: 0.4707 - accuracy: 0.7956 - val_loss: 0.4581 - val_accuracy: 0.8025\n",
            "Epoch 8/25\n",
            "80/80 [==============================] - 0s 5ms/step - loss: 0.4659 - accuracy: 0.7931 - val_loss: 0.4536 - val_accuracy: 0.8025\n",
            "Epoch 9/25\n",
            "80/80 [==============================] - 0s 5ms/step - loss: 0.4623 - accuracy: 0.7962 - val_loss: 0.4556 - val_accuracy: 0.7887\n",
            "Epoch 10/25\n",
            "80/80 [==============================] - 0s 5ms/step - loss: 0.4668 - accuracy: 0.7883 - val_loss: 0.4487 - val_accuracy: 0.8018\n",
            "Epoch 11/25\n",
            "80/80 [==============================] - 0s 6ms/step - loss: 0.4571 - accuracy: 0.7956 - val_loss: 0.4470 - val_accuracy: 0.8029\n",
            "Epoch 12/25\n",
            "80/80 [==============================] - 0s 6ms/step - loss: 0.4539 - accuracy: 0.7947 - val_loss: 0.4419 - val_accuracy: 0.8079\n",
            "Epoch 13/25\n",
            "80/80 [==============================] - 0s 5ms/step - loss: 0.4518 - accuracy: 0.7947 - val_loss: 0.4516 - val_accuracy: 0.7972\n",
            "Epoch 14/25\n",
            "80/80 [==============================] - 0s 5ms/step - loss: 0.4526 - accuracy: 0.7941 - val_loss: 0.4427 - val_accuracy: 0.8067\n",
            "Epoch 15/25\n",
            "80/80 [==============================] - 0s 5ms/step - loss: 0.4501 - accuracy: 0.7954 - val_loss: 0.4396 - val_accuracy: 0.8041\n",
            "Epoch 16/25\n",
            "80/80 [==============================] - 0s 5ms/step - loss: 0.4472 - accuracy: 0.7967 - val_loss: 0.4419 - val_accuracy: 0.8006\n",
            "Epoch 17/25\n",
            "80/80 [==============================] - 0s 5ms/step - loss: 0.4474 - accuracy: 0.7941 - val_loss: 0.4402 - val_accuracy: 0.8064\n",
            "Epoch 18/25\n",
            "80/80 [==============================] - 0s 5ms/step - loss: 0.4448 - accuracy: 0.7961 - val_loss: 0.4400 - val_accuracy: 0.8033\n",
            "Epoch 19/25\n",
            "80/80 [==============================] - 0s 6ms/step - loss: 0.4466 - accuracy: 0.7961 - val_loss: 0.4412 - val_accuracy: 0.8025\n",
            "Epoch 20/25\n",
            "80/80 [==============================] - 0s 5ms/step - loss: 0.4475 - accuracy: 0.7929 - val_loss: 0.4335 - val_accuracy: 0.8094\n",
            "Epoch 21/25\n",
            "80/80 [==============================] - 0s 5ms/step - loss: 0.4477 - accuracy: 0.7895 - val_loss: 0.4352 - val_accuracy: 0.8064\n",
            "Epoch 22/25\n",
            "80/80 [==============================] - 0s 5ms/step - loss: 0.4417 - accuracy: 0.7957 - val_loss: 0.4343 - val_accuracy: 0.8041\n",
            "Epoch 23/25\n",
            "80/80 [==============================] - 0s 5ms/step - loss: 0.4396 - accuracy: 0.7965 - val_loss: 0.4340 - val_accuracy: 0.8060\n",
            "Epoch 24/25\n",
            "80/80 [==============================] - 0s 5ms/step - loss: 0.4443 - accuracy: 0.7946 - val_loss: 0.4325 - val_accuracy: 0.8098\n",
            "Epoch 25/25\n",
            "80/80 [==============================] - 0s 5ms/step - loss: 0.4375 - accuracy: 0.7987 - val_loss: 0.4325 - val_accuracy: 0.8044\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f57d620d650>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ensemble"
      ],
      "metadata": {
        "id": "OVoxz3aNAG5R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "install_catboost = True\n",
        "if install_catboost:\n",
        "  !pip install catboost\n",
        "  !pip install ipywidgets\n",
        "  !pip install lightgbm "
      ],
      "metadata": {
        "id": "6R3OqzQ0I50e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2c3c1f2-492a-4d57-8e64-8f572ff85a60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting catboost\n",
            "  Downloading catboost-1.1.1-cp37-none-manylinux1_x86_64.whl (76.6 MB)\n",
            "\u001b[K     || 76.6 MB 1.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from catboost) (1.21.6)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.7/dist-packages (from catboost) (0.10.1)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.7/dist-packages (from catboost) (5.5.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from catboost) (1.15.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from catboost) (1.7.3)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.7/dist-packages (from catboost) (1.3.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from catboost) (3.2.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->catboost) (2022.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->catboost) (2.8.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->catboost) (3.0.9)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->catboost) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->catboost) (1.4.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib->catboost) (4.1.1)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from plotly->catboost) (8.1.0)\n",
            "Installing collected packages: catboost\n",
            "Successfully installed catboost-1.1.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.7/dist-packages (7.7.1)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets) (3.0.3)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets) (3.6.1)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets) (7.9.0)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets) (0.2.0)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.7/dist-packages (from ipywidgets) (5.3.4)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.7/dist-packages (from ipywidgets) (5.1.1)\n",
            "Requirement already satisfied: tornado>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=4.5.1->ipywidgets) (5.1.1)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.7/dist-packages (from ipykernel>=4.5.1->ipywidgets) (6.1.12)\n",
            "Collecting jedi>=0.10\n",
            "  Downloading jedi-0.18.1-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[K     || 1.6 MB 4.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets) (2.6.1)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets) (0.7.5)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets) (4.8.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets) (0.2.0)\n",
            "Requirement already satisfied: prompt-toolkit<2.1.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets) (2.0.10)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets) (4.4.2)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets) (57.4.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from jedi>=0.10->ipython>=4.0.0->ipywidgets) (0.8.3)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython>=4.0.0->ipywidgets) (1.15.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython>=4.0.0->ipywidgets) (0.2.5)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.7/dist-packages (from widgetsnbextension~=3.6.0->ipywidgets) (5.5.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.11.3)\n",
            "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (23.2.1)\n",
            "Requirement already satisfied: jupyter-core>=4.4.0 in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (4.11.2)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.8.0)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (5.6.1)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (5.7.0)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.13.3)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets) (2.8.2)\n",
            "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.7/dist-packages (from terminado>=0.8.1->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.7.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.0.1)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.7.1)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (5.0.1)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.8.4)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.5.0)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.4)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.6.0)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.7/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (4.3.3)\n",
            "Requirement already satisfied: importlib-metadata>=3.6 in /usr/local/lib/python3.7/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (4.13.0)\n",
            "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.7/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.16.2)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=3.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=3.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (3.10.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.18.1)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (22.1.0)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (5.10.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.5.1)\n",
            "Installing collected packages: jedi\n",
            "Successfully installed jedi-0.18.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: lightgbm in /usr/local/lib/python3.7/dist-packages (2.2.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from lightgbm) (1.21.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from lightgbm) (1.7.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from lightgbm) (1.0.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->lightgbm) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->lightgbm) (1.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from catboost import CatBoostClassifier\n",
        "import lightgbm\n",
        "from lightgbm import LGBMClassifier\n",
        "import time\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "use_probabilities = False\n",
        "\n",
        "# Classifiers\n",
        "classifiers = {\n",
        "    \"LogisticRegression\" : LogisticRegression(C=1, penalty='l1', solver='liblinear'),\n",
        "    \"KNN\" : KNeighborsClassifier(n_neighbors=9, p=2),\n",
        "    \"SVC\" : SVC(C= 1.25, gamma= 'scale', kernel= 'rbf',probability=use_probabilities),\n",
        "    \"RandomForest\" : RandomForestClassifier(max_depth= 10, n_estimators= 250),\n",
        "    \"LGBM\" : LGBMClassifier(learning_rate= 0.05, max_depth= 8, n_estimators= 100),\n",
        "    \"CatBoost\" : CatBoostClassifier(learning_rate= 0.15, max_depth= 4, n_estimators= 100),\n",
        "    \"NaiveBayes\": GaussianNB(var_smoothing= 1e-07)\n",
        "}\n",
        "\n",
        "# Grids for grid search\n",
        "LR_grid = {'penalty': ['l1','l2'],\n",
        "           'C': [0.25, 0.5, 0.75, 1, 1.25, 1.5],\n",
        "           'max_iter': [50, 100, 150]}\n",
        "\n",
        "KNN_grid = {'n_neighbors': [3, 5, 7, 9],\n",
        "            'p': [1, 2]}\n",
        "\n",
        "SVC_grid = {'C': [0.25, 0.5, 0.75, 1, 1.25, 1.5],\n",
        "            'kernel': ['linear', 'rbf'],\n",
        "            'gamma': ['scale', 'auto']}\n",
        "\n",
        "RF_grid = {'n_estimators': [50, 100, 150, 200, 250, 300],\n",
        "        'max_depth': [4, 6, 8, 10, 12]}\n",
        "\n",
        "boosted_grid = {'n_estimators': [50, 100, 150, 200],\n",
        "        'max_depth': [4, 8, 12],\n",
        "        'learning_rate': [0.05, 0.1, 0.15]}\n",
        "\n",
        "NB_grid={'var_smoothing': [1e-10, 1e-9, 1e-8, 1e-7]}\n",
        "\n",
        "# Dictionary of all grids\n",
        "grid = {\n",
        "    \"LogisticRegression\" : LR_grid,\n",
        "    \"KNN\" : KNN_grid,\n",
        "    \"SVC\" : SVC_grid,\n",
        "    \"RandomForest\" : RF_grid,\n",
        "    \"XGBoost\" : boosted_grid,\n",
        "    \"LGBM\" : boosted_grid,\n",
        "    \"CatBoost\" : boosted_grid,\n",
        "    \"NaiveBayes\": NB_grid\n",
        "}"
      ],
      "metadata": {
        "id": "83GmqLLzAJ68"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluate models using accuracy, models are evaluated on X_test which is the validation dataset\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "i=0\n",
        "clf_best_params=classifiers.copy()\n",
        "valid_scores=pd.DataFrame({'Classifer':classifiers.keys(), 'Validation accuracy': np.zeros(len(classifiers)), 'Training time': np.zeros(len(classifiers))})\n",
        "all_predictions={}\n",
        "for key, classifier in classifiers.items():\n",
        "    start = time.time()\n",
        "    #clf = GridSearchCV(estimator=classifier, param_grid=grid[key], n_jobs=-1, cv=None)\n",
        "    clf = classifier\n",
        "    # Train and score\n",
        "    clf.fit(X_train, y_train)\n",
        "    print(key)\n",
        "    \n",
        "    if use_probabilities:\n",
        "      y_predictions = clf.predict_proba(X_test)\n",
        "      y_predictions = [el[1] for el in y_predictions]\n",
        "    else:\n",
        "      y_predictions = clf.predict(X_test)\n",
        "\n",
        "\n",
        "    all_predictions[key] = y_predictions\n",
        "    var_score = [1 if prediction>0.5 else 0 for prediction in y_predictions ]\n",
        "\n",
        "    valid_scores.iloc[i,1]=accuracy_score(y_test, var_score)\n",
        "\n",
        "    \n",
        "    # Print iteration and training time\n",
        "    stop = time.time()\n",
        "    valid_scores.iloc[i,2]=np.round((stop - start)/60, 2)\n",
        "    \n",
        "    print('Model:', key)\n",
        "    print('Training time (mins):', valid_scores.iloc[i,2])\n",
        "    print('')\n",
        "    i+=1\n",
        "print(y_predictions)\n",
        "print(valid_scores)"
      ],
      "metadata": {
        "id": "9OFfy4m8CBUb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11c749d4-2fb7-4537-e118-f00dfb301b7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LogisticRegression\n",
            "Model: LogisticRegression\n",
            "Training time (mins): 0.0\n",
            "\n",
            "KNN\n",
            "Model: KNN\n",
            "Training time (mins): 0.0\n",
            "\n",
            "SVC\n",
            "Model: SVC\n",
            "Training time (mins): 0.07\n",
            "\n",
            "RandomForest\n",
            "Model: RandomForest\n",
            "Training time (mins): 0.04\n",
            "\n",
            "LGBM\n",
            "Model: LGBM\n",
            "Training time (mins): 0.02\n",
            "\n",
            "0:\tlearn: 0.6536250\ttotal: 56.6ms\tremaining: 5.61s\n",
            "1:\tlearn: 0.6234058\ttotal: 61.8ms\tremaining: 3.03s\n",
            "2:\tlearn: 0.5994085\ttotal: 65.9ms\tremaining: 2.13s\n",
            "3:\tlearn: 0.5820586\ttotal: 69.8ms\tremaining: 1.68s\n",
            "4:\tlearn: 0.5676408\ttotal: 73.1ms\tremaining: 1.39s\n",
            "5:\tlearn: 0.5544358\ttotal: 79.4ms\tremaining: 1.24s\n",
            "6:\tlearn: 0.5446050\ttotal: 84.1ms\tremaining: 1.12s\n",
            "7:\tlearn: 0.5337131\ttotal: 86.1ms\tremaining: 991ms\n",
            "8:\tlearn: 0.5253804\ttotal: 89.9ms\tremaining: 909ms\n",
            "9:\tlearn: 0.5191622\ttotal: 94ms\tremaining: 846ms\n",
            "10:\tlearn: 0.5116249\ttotal: 98.7ms\tremaining: 799ms\n",
            "11:\tlearn: 0.5047054\ttotal: 102ms\tremaining: 749ms\n",
            "12:\tlearn: 0.4986752\ttotal: 106ms\tremaining: 711ms\n",
            "13:\tlearn: 0.4934857\ttotal: 122ms\tremaining: 750ms\n",
            "14:\tlearn: 0.4896022\ttotal: 127ms\tremaining: 718ms\n",
            "15:\tlearn: 0.4859399\ttotal: 130ms\tremaining: 684ms\n",
            "16:\tlearn: 0.4826351\ttotal: 135ms\tremaining: 658ms\n",
            "17:\tlearn: 0.4795551\ttotal: 138ms\tremaining: 628ms\n",
            "18:\tlearn: 0.4769712\ttotal: 142ms\tremaining: 605ms\n",
            "19:\tlearn: 0.4743895\ttotal: 146ms\tremaining: 584ms\n",
            "20:\tlearn: 0.4714876\ttotal: 150ms\tremaining: 564ms\n",
            "21:\tlearn: 0.4687263\ttotal: 154ms\tremaining: 546ms\n",
            "22:\tlearn: 0.4663163\ttotal: 158ms\tremaining: 529ms\n",
            "23:\tlearn: 0.4641970\ttotal: 162ms\tremaining: 513ms\n",
            "24:\tlearn: 0.4625109\ttotal: 168ms\tremaining: 503ms\n",
            "25:\tlearn: 0.4609685\ttotal: 171ms\tremaining: 486ms\n",
            "26:\tlearn: 0.4599300\ttotal: 174ms\tremaining: 471ms\n",
            "27:\tlearn: 0.4585051\ttotal: 178ms\tremaining: 458ms\n",
            "28:\tlearn: 0.4572607\ttotal: 182ms\tremaining: 445ms\n",
            "29:\tlearn: 0.4564772\ttotal: 187ms\tremaining: 436ms\n",
            "30:\tlearn: 0.4554203\ttotal: 190ms\tremaining: 423ms\n",
            "31:\tlearn: 0.4543189\ttotal: 194ms\tremaining: 412ms\n",
            "32:\tlearn: 0.4534924\ttotal: 198ms\tremaining: 403ms\n",
            "33:\tlearn: 0.4523160\ttotal: 203ms\tremaining: 393ms\n",
            "34:\tlearn: 0.4515640\ttotal: 207ms\tremaining: 384ms\n",
            "35:\tlearn: 0.4497133\ttotal: 211ms\tremaining: 375ms\n",
            "36:\tlearn: 0.4492131\ttotal: 215ms\tremaining: 366ms\n",
            "37:\tlearn: 0.4485356\ttotal: 219ms\tremaining: 357ms\n",
            "38:\tlearn: 0.4473895\ttotal: 223ms\tremaining: 349ms\n",
            "39:\tlearn: 0.4465295\ttotal: 227ms\tremaining: 340ms\n",
            "40:\tlearn: 0.4460557\ttotal: 231ms\tremaining: 332ms\n",
            "41:\tlearn: 0.4452075\ttotal: 237ms\tremaining: 327ms\n",
            "42:\tlearn: 0.4447935\ttotal: 239ms\tremaining: 317ms\n",
            "43:\tlearn: 0.4439670\ttotal: 243ms\tremaining: 310ms\n",
            "44:\tlearn: 0.4428505\ttotal: 247ms\tremaining: 302ms\n",
            "45:\tlearn: 0.4423159\ttotal: 251ms\tremaining: 295ms\n",
            "46:\tlearn: 0.4420250\ttotal: 255ms\tremaining: 288ms\n",
            "47:\tlearn: 0.4416362\ttotal: 260ms\tremaining: 281ms\n",
            "48:\tlearn: 0.4411498\ttotal: 264ms\tremaining: 275ms\n",
            "49:\tlearn: 0.4406480\ttotal: 268ms\tremaining: 268ms\n",
            "50:\tlearn: 0.4401067\ttotal: 272ms\tremaining: 262ms\n",
            "51:\tlearn: 0.4396404\ttotal: 276ms\tremaining: 255ms\n",
            "52:\tlearn: 0.4391495\ttotal: 280ms\tremaining: 249ms\n",
            "53:\tlearn: 0.4386102\ttotal: 284ms\tremaining: 242ms\n",
            "54:\tlearn: 0.4380410\ttotal: 296ms\tremaining: 242ms\n",
            "55:\tlearn: 0.4368964\ttotal: 300ms\tremaining: 235ms\n",
            "56:\tlearn: 0.4364036\ttotal: 304ms\tremaining: 229ms\n",
            "57:\tlearn: 0.4361766\ttotal: 308ms\tremaining: 223ms\n",
            "58:\tlearn: 0.4357902\ttotal: 312ms\tremaining: 217ms\n",
            "59:\tlearn: 0.4353704\ttotal: 318ms\tremaining: 212ms\n",
            "60:\tlearn: 0.4351043\ttotal: 322ms\tremaining: 206ms\n",
            "61:\tlearn: 0.4346326\ttotal: 326ms\tremaining: 200ms\n",
            "62:\tlearn: 0.4340349\ttotal: 330ms\tremaining: 194ms\n",
            "63:\tlearn: 0.4336107\ttotal: 334ms\tremaining: 188ms\n",
            "64:\tlearn: 0.4328919\ttotal: 338ms\tremaining: 182ms\n",
            "65:\tlearn: 0.4325620\ttotal: 342ms\tremaining: 176ms\n",
            "66:\tlearn: 0.4322020\ttotal: 346ms\tremaining: 170ms\n",
            "67:\tlearn: 0.4319386\ttotal: 350ms\tremaining: 165ms\n",
            "68:\tlearn: 0.4312376\ttotal: 354ms\tremaining: 159ms\n",
            "69:\tlearn: 0.4307950\ttotal: 356ms\tremaining: 152ms\n",
            "70:\tlearn: 0.4303268\ttotal: 358ms\tremaining: 146ms\n",
            "71:\tlearn: 0.4300950\ttotal: 359ms\tremaining: 140ms\n",
            "72:\tlearn: 0.4298209\ttotal: 360ms\tremaining: 133ms\n",
            "73:\tlearn: 0.4296428\ttotal: 362ms\tremaining: 127ms\n",
            "74:\tlearn: 0.4284650\ttotal: 363ms\tremaining: 121ms\n",
            "75:\tlearn: 0.4282078\ttotal: 365ms\tremaining: 115ms\n",
            "76:\tlearn: 0.4278892\ttotal: 366ms\tremaining: 109ms\n",
            "77:\tlearn: 0.4275386\ttotal: 367ms\tremaining: 104ms\n",
            "78:\tlearn: 0.4272122\ttotal: 369ms\tremaining: 98.1ms\n",
            "79:\tlearn: 0.4262775\ttotal: 371ms\tremaining: 92.8ms\n",
            "80:\tlearn: 0.4260632\ttotal: 372ms\tremaining: 87.4ms\n",
            "81:\tlearn: 0.4257579\ttotal: 374ms\tremaining: 82ms\n",
            "82:\tlearn: 0.4255124\ttotal: 375ms\tremaining: 76.8ms\n",
            "83:\tlearn: 0.4251557\ttotal: 376ms\tremaining: 71.7ms\n",
            "84:\tlearn: 0.4247761\ttotal: 378ms\tremaining: 66.7ms\n",
            "85:\tlearn: 0.4245710\ttotal: 379ms\tremaining: 61.7ms\n",
            "86:\tlearn: 0.4243987\ttotal: 380ms\tremaining: 56.9ms\n",
            "87:\tlearn: 0.4241378\ttotal: 382ms\tremaining: 52.1ms\n",
            "88:\tlearn: 0.4238423\ttotal: 384ms\tremaining: 47.5ms\n",
            "89:\tlearn: 0.4233582\ttotal: 385ms\tremaining: 42.8ms\n",
            "90:\tlearn: 0.4216304\ttotal: 387ms\tremaining: 38.3ms\n",
            "91:\tlearn: 0.4207819\ttotal: 388ms\tremaining: 33.8ms\n",
            "92:\tlearn: 0.4205483\ttotal: 390ms\tremaining: 29.3ms\n",
            "93:\tlearn: 0.4203824\ttotal: 391ms\tremaining: 25ms\n",
            "94:\tlearn: 0.4190939\ttotal: 393ms\tremaining: 20.7ms\n",
            "95:\tlearn: 0.4187878\ttotal: 394ms\tremaining: 16.4ms\n",
            "96:\tlearn: 0.4186365\ttotal: 396ms\tremaining: 12.3ms\n",
            "97:\tlearn: 0.4184687\ttotal: 398ms\tremaining: 8.11ms\n",
            "98:\tlearn: 0.4182272\ttotal: 399ms\tremaining: 4.03ms\n",
            "99:\tlearn: 0.4180130\ttotal: 400ms\tremaining: 0us\n",
            "CatBoost\n",
            "Model: CatBoost\n",
            "Training time (mins): 0.01\n",
            "\n",
            "NaiveBayes\n",
            "Model: NaiveBayes\n",
            "Training time (mins): 0.0\n",
            "\n",
            "[1. 1. 1. ... 1. 1. 1.]\n",
            "            Classifer  Validation accuracy  Training time\n",
            "0  LogisticRegression             0.793712           0.00\n",
            "1                 KNN             0.749617           0.00\n",
            "2                 SVC             0.800997           0.07\n",
            "3        RandomForest             0.807132           0.04\n",
            "4                LGBM             0.804448           0.02\n",
            "5            CatBoost             0.803681           0.01\n",
            "6          NaiveBayes             0.662577           0.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "valid_scores"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "id": "K0MRdv-umGX6",
        "outputId": "fa71bd0d-9206-416a-b0b5-8337e9d61990"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            Classifer  Validation accuracy  Training time\n",
              "0  LogisticRegression             0.793712           0.00\n",
              "1                 KNN             0.749617           0.00\n",
              "2                 SVC             0.800997           0.07\n",
              "3        RandomForest             0.807132           0.04\n",
              "4                LGBM             0.804448           0.02\n",
              "5            CatBoost             0.803681           0.01\n",
              "6          NaiveBayes             0.662577           0.00"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8cab205f-0340-433c-8fe1-237c3121e568\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Classifer</th>\n",
              "      <th>Validation accuracy</th>\n",
              "      <th>Training time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LogisticRegression</td>\n",
              "      <td>0.793712</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>KNN</td>\n",
              "      <td>0.749617</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>SVC</td>\n",
              "      <td>0.800997</td>\n",
              "      <td>0.07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>RandomForest</td>\n",
              "      <td>0.807132</td>\n",
              "      <td>0.04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>LGBM</td>\n",
              "      <td>0.804448</td>\n",
              "      <td>0.02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>CatBoost</td>\n",
              "      <td>0.803681</td>\n",
              "      <td>0.01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>NaiveBayes</td>\n",
              "      <td>0.662577</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8cab205f-0340-433c-8fe1-237c3121e568')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-8cab205f-0340-433c-8fe1-237c3121e568 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-8cab205f-0340-433c-8fe1-237c3121e568');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_predictions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dZpkwqlWXZXR",
        "outputId": "da413d4e-2212-4e03-c9ca-9125c966113c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'LogisticRegression': array([1., 1., 1., ..., 1., 1., 0.]),\n",
              " 'KNN': array([0., 1., 1., ..., 0., 1., 0.]),\n",
              " 'SVC': array([1., 1., 1., ..., 1., 1., 0.]),\n",
              " 'RandomForest': array([1., 1., 1., ..., 1., 1., 1.]),\n",
              " 'LGBM': array([1., 1., 1., ..., 1., 1., 0.]),\n",
              " 'CatBoost': array([1., 1., 1., ..., 1., 1., 0.]),\n",
              " 'NaiveBayes': array([1., 1., 1., ..., 1., 1., 1.])}"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Predictions using ensemble - 5 algorithms\n",
        "y_predictions_tf = model.predict(X_test)\n",
        "all_predictions['tf'] = np.array([1 if prediction>0.5 else 0 for prediction in y_predictions_tf ])\n",
        "\n",
        "#5 models will be used, this problem is of binary classification, so result of prediction is either 0 or 1\n",
        "#if the sum of the 5 models prediction is 3 or higher it means that a majority of the models predicted that the passengers were transported\n",
        "best_predictions ={}\n",
        "best_predictions['RandomForest'] = all_predictions['RandomForest']\n",
        "best_predictions['LGBM'] = all_predictions['LGBM']\n",
        "best_predictions['tf'] = all_predictions['tf']\n",
        "best_predictions['CatBoost'] = all_predictions['CatBoost']\n",
        "best_predictions['SVC'] = all_predictions['SVC']\n",
        "\n",
        "sum_predictions = np.zeros(len(best_predictions['tf']))\n",
        "for pred in best_predictions.values():\n",
        "  for i, v in enumerate(pred):\n",
        "    sum_predictions[i] += v\n",
        "\n",
        "\n",
        "ensemble_predictions = np.array([1 if s>=3 else 0 for s in sum_predictions ])\n",
        "ensemble_predictions\n",
        "print('ensemble accuracy on validation set is ', str(accuracy_score(y_test, ensemble_predictions)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DamIn2dwJCJU",
        "outputId": "e1c12025-6d38-47bd-c6d1-567fc1990026"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "82/82 [==============================] - 0s 2ms/step\n",
            "ensemble accuracy on validation set 0.8052147239263804\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sum_predictions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QyPOyqZhlbYJ",
        "outputId": "af75ff92-f966-4c5a-c4d4-be57ca49a50e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([5., 5., 5., ..., 5., 5., 1.])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('tensorflow accuracy on validation set is ', str(accuracy_score(y_test, all_predictions['tf'])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5EY0T089jLXb",
        "outputId": "baa95fc7-2978-4456-b466-ed72bb35ac55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensorflow accuracy on validation set is  0.8044478527607362\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Predict on the \"real\" test set, which is df_test2, true results are not know for df_test2\n",
        "predictions_ensemble_final={}\n",
        "\n",
        "if 'Transported' in vars_to_use_training:\n",
        "    vars_to_use_training.remove('Transported')\n",
        "\n",
        "df_test2 = df_test2[vars_to_use_training]\n",
        "\n",
        "#predict for algorithms\n",
        "for key, classifier in classifiers.items():\n",
        "    clf = classifier\n",
        "    clf.fit(X_train, y_train)\n",
        "    \n",
        "    clf_predictions = clf.predict(df_test2)\n",
        "    predictions_ensemble_final[key] = clf_predictions\n",
        "\n",
        "#predict for tensorflow\n",
        "y_predictions_tf = model.predict(df_test2)\n",
        "predictions_ensemble_final['tf'] = np.array([1 if prediction>0.5 else 0 for prediction in y_predictions_tf ])"
      ],
      "metadata": {
        "id": "oheBenwOnAS6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d6704c4-d669-4fd8-c8d0-c61b8295e817"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0:\tlearn: 0.6536250\ttotal: 1.5ms\tremaining: 149ms\n",
            "1:\tlearn: 0.6234058\ttotal: 3.63ms\tremaining: 178ms\n",
            "2:\tlearn: 0.5994085\ttotal: 5.29ms\tremaining: 171ms\n",
            "3:\tlearn: 0.5820586\ttotal: 6.98ms\tremaining: 168ms\n",
            "4:\tlearn: 0.5676408\ttotal: 8.52ms\tremaining: 162ms\n",
            "5:\tlearn: 0.5544358\ttotal: 10.3ms\tremaining: 161ms\n",
            "6:\tlearn: 0.5446050\ttotal: 12.1ms\tremaining: 160ms\n",
            "7:\tlearn: 0.5337131\ttotal: 13.7ms\tremaining: 158ms\n",
            "8:\tlearn: 0.5253804\ttotal: 15.4ms\tremaining: 156ms\n",
            "9:\tlearn: 0.5191622\ttotal: 17.3ms\tremaining: 155ms\n",
            "10:\tlearn: 0.5116249\ttotal: 18.9ms\tremaining: 153ms\n",
            "11:\tlearn: 0.5047054\ttotal: 20.5ms\tremaining: 151ms\n",
            "12:\tlearn: 0.4986752\ttotal: 22.2ms\tremaining: 148ms\n",
            "13:\tlearn: 0.4934857\ttotal: 23.9ms\tremaining: 147ms\n",
            "14:\tlearn: 0.4896022\ttotal: 25.3ms\tremaining: 144ms\n",
            "15:\tlearn: 0.4859399\ttotal: 27.2ms\tremaining: 143ms\n",
            "16:\tlearn: 0.4826351\ttotal: 30ms\tremaining: 147ms\n",
            "17:\tlearn: 0.4795551\ttotal: 32ms\tremaining: 146ms\n",
            "18:\tlearn: 0.4769712\ttotal: 36ms\tremaining: 154ms\n",
            "19:\tlearn: 0.4743895\ttotal: 37.7ms\tremaining: 151ms\n",
            "20:\tlearn: 0.4714876\ttotal: 39.3ms\tremaining: 148ms\n",
            "21:\tlearn: 0.4687263\ttotal: 40.9ms\tremaining: 145ms\n",
            "22:\tlearn: 0.4663163\ttotal: 42.5ms\tremaining: 142ms\n",
            "23:\tlearn: 0.4641970\ttotal: 44.3ms\tremaining: 140ms\n",
            "24:\tlearn: 0.4625109\ttotal: 45.7ms\tremaining: 137ms\n",
            "25:\tlearn: 0.4609685\ttotal: 47.2ms\tremaining: 134ms\n",
            "26:\tlearn: 0.4599300\ttotal: 48.9ms\tremaining: 132ms\n",
            "27:\tlearn: 0.4585051\ttotal: 50.5ms\tremaining: 130ms\n",
            "28:\tlearn: 0.4572607\ttotal: 52.1ms\tremaining: 127ms\n",
            "29:\tlearn: 0.4564772\ttotal: 53.7ms\tremaining: 125ms\n",
            "30:\tlearn: 0.4554203\ttotal: 55.3ms\tremaining: 123ms\n",
            "31:\tlearn: 0.4543189\ttotal: 56.9ms\tremaining: 121ms\n",
            "32:\tlearn: 0.4534924\ttotal: 58.6ms\tremaining: 119ms\n",
            "33:\tlearn: 0.4523160\ttotal: 60.2ms\tremaining: 117ms\n",
            "34:\tlearn: 0.4515640\ttotal: 61.8ms\tremaining: 115ms\n",
            "35:\tlearn: 0.4497133\ttotal: 63.4ms\tremaining: 113ms\n",
            "36:\tlearn: 0.4492131\ttotal: 65ms\tremaining: 111ms\n",
            "37:\tlearn: 0.4485356\ttotal: 66.6ms\tremaining: 109ms\n",
            "38:\tlearn: 0.4473895\ttotal: 68.2ms\tremaining: 107ms\n",
            "39:\tlearn: 0.4465295\ttotal: 69.8ms\tremaining: 105ms\n",
            "40:\tlearn: 0.4460557\ttotal: 71.3ms\tremaining: 103ms\n",
            "41:\tlearn: 0.4452075\ttotal: 73ms\tremaining: 101ms\n",
            "42:\tlearn: 0.4447935\ttotal: 74.4ms\tremaining: 98.6ms\n",
            "43:\tlearn: 0.4439670\ttotal: 76ms\tremaining: 96.7ms\n",
            "44:\tlearn: 0.4428505\ttotal: 77.5ms\tremaining: 94.7ms\n",
            "45:\tlearn: 0.4423159\ttotal: 79.2ms\tremaining: 93ms\n",
            "46:\tlearn: 0.4420250\ttotal: 80.9ms\tremaining: 91.2ms\n",
            "47:\tlearn: 0.4416362\ttotal: 82.4ms\tremaining: 89.3ms\n",
            "48:\tlearn: 0.4411498\ttotal: 84.1ms\tremaining: 87.5ms\n",
            "49:\tlearn: 0.4406480\ttotal: 85.8ms\tremaining: 85.8ms\n",
            "50:\tlearn: 0.4401067\ttotal: 87.4ms\tremaining: 84ms\n",
            "51:\tlearn: 0.4396404\ttotal: 89.2ms\tremaining: 82.3ms\n",
            "52:\tlearn: 0.4391495\ttotal: 90.5ms\tremaining: 80.3ms\n",
            "53:\tlearn: 0.4386102\ttotal: 92.1ms\tremaining: 78.4ms\n",
            "54:\tlearn: 0.4380410\ttotal: 93.9ms\tremaining: 76.8ms\n",
            "55:\tlearn: 0.4368964\ttotal: 95.6ms\tremaining: 75.1ms\n",
            "56:\tlearn: 0.4364036\ttotal: 97.1ms\tremaining: 73.3ms\n",
            "57:\tlearn: 0.4361766\ttotal: 98.9ms\tremaining: 71.6ms\n",
            "58:\tlearn: 0.4357902\ttotal: 101ms\tremaining: 69.9ms\n",
            "59:\tlearn: 0.4353704\ttotal: 102ms\tremaining: 68.2ms\n",
            "60:\tlearn: 0.4351043\ttotal: 104ms\tremaining: 66.4ms\n",
            "61:\tlearn: 0.4346326\ttotal: 106ms\tremaining: 64.7ms\n",
            "62:\tlearn: 0.4340349\ttotal: 107ms\tremaining: 63ms\n",
            "63:\tlearn: 0.4336107\ttotal: 109ms\tremaining: 61.2ms\n",
            "64:\tlearn: 0.4328919\ttotal: 110ms\tremaining: 59.4ms\n",
            "65:\tlearn: 0.4325620\ttotal: 112ms\tremaining: 57.7ms\n",
            "66:\tlearn: 0.4322020\ttotal: 115ms\tremaining: 56.4ms\n",
            "67:\tlearn: 0.4319386\ttotal: 116ms\tremaining: 54.7ms\n",
            "68:\tlearn: 0.4312376\ttotal: 118ms\tremaining: 53ms\n",
            "69:\tlearn: 0.4307950\ttotal: 120ms\tremaining: 51.3ms\n",
            "70:\tlearn: 0.4303268\ttotal: 121ms\tremaining: 49.5ms\n",
            "71:\tlearn: 0.4300950\ttotal: 123ms\tremaining: 47.9ms\n",
            "72:\tlearn: 0.4298209\ttotal: 124ms\tremaining: 46ms\n",
            "73:\tlearn: 0.4296428\ttotal: 126ms\tremaining: 44.3ms\n",
            "74:\tlearn: 0.4284650\ttotal: 128ms\tremaining: 42.6ms\n",
            "75:\tlearn: 0.4282078\ttotal: 129ms\tremaining: 40.8ms\n",
            "76:\tlearn: 0.4278892\ttotal: 130ms\tremaining: 39ms\n",
            "77:\tlearn: 0.4275386\ttotal: 132ms\tremaining: 37.3ms\n",
            "78:\tlearn: 0.4272122\ttotal: 134ms\tremaining: 35.5ms\n",
            "79:\tlearn: 0.4262775\ttotal: 135ms\tremaining: 33.8ms\n",
            "80:\tlearn: 0.4260632\ttotal: 137ms\tremaining: 32.1ms\n",
            "81:\tlearn: 0.4257579\ttotal: 138ms\tremaining: 30.4ms\n",
            "82:\tlearn: 0.4255124\ttotal: 140ms\tremaining: 28.6ms\n",
            "83:\tlearn: 0.4251557\ttotal: 141ms\tremaining: 26.9ms\n",
            "84:\tlearn: 0.4247761\ttotal: 143ms\tremaining: 25.2ms\n",
            "85:\tlearn: 0.4245710\ttotal: 144ms\tremaining: 23.5ms\n",
            "86:\tlearn: 0.4243987\ttotal: 146ms\tremaining: 21.8ms\n",
            "87:\tlearn: 0.4241378\ttotal: 147ms\tremaining: 20.1ms\n",
            "88:\tlearn: 0.4238423\ttotal: 149ms\tremaining: 18.4ms\n",
            "89:\tlearn: 0.4233582\ttotal: 151ms\tremaining: 16.7ms\n",
            "90:\tlearn: 0.4216304\ttotal: 153ms\tremaining: 15.1ms\n",
            "91:\tlearn: 0.4207819\ttotal: 154ms\tremaining: 13.4ms\n",
            "92:\tlearn: 0.4205483\ttotal: 156ms\tremaining: 11.7ms\n",
            "93:\tlearn: 0.4203824\ttotal: 158ms\tremaining: 10.1ms\n",
            "94:\tlearn: 0.4190939\ttotal: 159ms\tremaining: 8.37ms\n",
            "95:\tlearn: 0.4187878\ttotal: 161ms\tremaining: 6.69ms\n",
            "96:\tlearn: 0.4186365\ttotal: 162ms\tremaining: 5.02ms\n",
            "97:\tlearn: 0.4184687\ttotal: 163ms\tremaining: 3.33ms\n",
            "98:\tlearn: 0.4182272\ttotal: 165ms\tremaining: 1.67ms\n",
            "99:\tlearn: 0.4180130\ttotal: 166ms\tremaining: 0us\n",
            "134/134 [==============================] - 0s 1ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#5 models will be used, this problem is of binary classification, so result of prediction is either 0 or 1\n",
        "#if the sum of the 5 models prediction is 3 or higher it means that a majority of the models predicted that the passengers were transported\n",
        "\n",
        "sub_predictions ={}\n",
        "sub_predictions['RandomForest'] = predictions_ensemble_final['RandomForest']\n",
        "sub_predictions['LGBM'] = predictions_ensemble_final['LGBM']\n",
        "sub_predictions['tf'] = predictions_ensemble_final['tf']\n",
        "sub_predictions['CatBoost'] = predictions_ensemble_final['CatBoost']\n",
        "sub_predictions['SVC'] = predictions_ensemble_final['SVC']\n",
        "\n",
        "sum_predictions = np.zeros(len(sub_predictions['tf']))\n",
        "for pred in sub_predictions.values():\n",
        "  for i, v in enumerate(pred):\n",
        "    sum_predictions[i] += v\n",
        "\n",
        "\n",
        "final_pred_ensemble = np.array([1 if s>=3 else 0 for s in sum_predictions ])\n",
        "final_pred_ensemble"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mYt9CZutn_k-",
        "outputId": "2b9e1cee-b8f8-4973-bfac-1c2de383dc5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 0, 1, ..., 1, 1, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if use_probabilities:\n",
        "  #Probabilities avg, instead of using 0 or 1 results, the probabilities of result\n",
        "  #for each model are gathered and the mean is calculated\n",
        "  y_predictions_tf = model.predict(X_test)\n",
        "  all_predictions_prob = all_predictions\n",
        "  all_predictions_prob['tf'] = y_predictions_tf\n",
        "\n",
        "  best_predictions_prob ={}\n",
        "  best_predictions_prob['RandomForest'] = all_predictions_prob['RandomForest']\n",
        "  best_predictions_prob['tf'] = all_predictions_prob['tf']\n",
        "\n",
        "  soma = np.zeros(len(best_predictions_prob['tf']))\n",
        "  for pred in best_predictions_prob.values():\n",
        "    for i, v in enumerate(pred):\n",
        "      soma[i] += v\n",
        "\n",
        "  number_models = len(best_predictions_prob)\n",
        "  pred_prob = [x / number_models for x in soma]\n",
        "  print(pred_prob)\n",
        "\n",
        "  ensemble_predictions_prob = np.array([1 if s>=0.5 else 0 for s in pred_prob])\n",
        "  ensemble_predictions_prob\n",
        "  print(accuracy_score(y_test, ensemble_predictions_prob))\n"
      ],
      "metadata": {
        "id": "ySumbqcGXxI7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(best_predictions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y9xQl9nCOA7W",
        "outputId": "ed92f9b3-c8b0-4000-e133-a0f2a01f10e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing"
      ],
      "metadata": {
        "id": "54MrsihZOxIj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if 'Transported' in vars_to_use_training:\n",
        "    vars_to_use_training.remove('Transported')\n",
        "df_test2 = df_test2[vars_to_use_training]\n",
        "\n",
        "y_predictions = model.predict(df_test2)\n",
        "print(df_test2.shape)\n",
        "\n"
      ],
      "metadata": {
        "id": "a3EIFC09OvKP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8b239e0-ae54-44eb-8a3c-2ac896d4ebd3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "134/134 [==============================] - 0s 1ms/step\n",
            "(4277, 7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if use_probabilities:\n",
        "  predictions_for_submission = ensemble_predictions_prob\n",
        "else:\n",
        "  predictions_for_submission = final_pred_ensemble\n",
        "\n",
        "var_score = ['True' if prediction>0.5 else 'False' for prediction in predictions_for_submission]\n",
        "print(var_score)\n",
        "\n",
        "score_df = pd.DataFrame(var_score)\n",
        "score_df.to_csv('score_df.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EFQpgWQeoWiJ",
        "outputId": "abf17e2d-fb4c-4a37-d214-4a8ca0523333"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['True', 'False', 'True', 'True', 'True', 'True', 'True', 'True', 'True', 'True', 'False', 'False', 'True', 'True', 'False', 'False', 'False', 'True', 'True', 'False', 'True', 'False', 'True', 'True', 'False', 'False', 'True', 'True', 'True', 'True', 'True', 'False', 'True', 'False', 'False', 'False', 'True', 'True', 'True', 'False', 'True', 'False', 'True', 'True', 'False', 'True', 'False', 'False', 'True', 'False', 'True', 'True', 'False', 'True', 'True', 'True', 'False', 'True', 'True', 'False', 'True', 'True', 'False', 'True', 'True', 'False', 'True', 'False', 'True', 'False', 'False', 'True', 'False', 'False', 'True', 'True', 'True', 'True', 'True', 'True', 'True', 'True', 'False', 'False', 'False', 'True', 'True', 'False', 'True', 'True', 'False', 'False', 'False', 'False', 'False', 'False', 'True', 'False', 'True', 'True', 'True', 'True', 'True', 'False', 'True', 'False', 'True', 'True', 'False', 'True', 'True', 'False', 'True', 'True', 'False', 'True', 'True', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'True', 'True', 'False', 'False', 'True', 'False', 'True', 'False', 'True', 'True', 'False', 'True', 'False', 'False', 'False', 'False', 'True', 'False', 'True', 'False', 'True', 'False', 'True', 'True', 'False', 'True', 'True', 'True', 'True', 'False', 'False', 'True', 'True', 'True', 'False', 'False', 'False', 'False', 'True', 'True', 'False', 'False', 'True', 'False', 'True', 'False', 'False', 'True', 'True', 'True', 'True', 'True', 'False', 'True', 'True', 'False', 'False', 'True', 'False', 'False', 'False', 'False', 'True', 'False', 'True', 'True', 'True', 'True', 'False', 'False', 'False', 'False', 'False', 'True', 'True', 'False', 'True', 'False', 'False', 'False', 'False', 'True', 'False', 'True', 'False', 'True', 'True', 'True', 'True', 'True', 'False', 'False', 'False', 'True', 'True', 'True', 'True', 'True', 'False', 'True', 'True', 'False', 'False', 'True', 'True', 'False', 'True', 'True', 'True', 'True', 'False', 'True', 'False', 'True', 'True', 'False', 'False', 'True', 'False', 'True', 'True', 'True', 'False', 'False', 'True', 'False', 'True', 'False', 'False', 'False', 'True', 'True', 'True', 'False', 'True', 'True', 'True', 'False', 'True', 'False', 'True', 'True', 'True', 'False', 'True', 'True', 'True', 'True', 'False', 'True', 'True', 'True', 'False', 'False', 'False', 'True', 'True', 'False', 'False', 'True', 'True', 'True', 'True', 'True', 'True', 'True', 'True', 'False', 'False', 'True', 'True', 'True', 'False', 'False', 'True', 'True', 'True', 'True', 'True', 'False', 'True', 'True', 'False', 'True', 'True', 'False', 'False', 'False', 'True', 'True', 'False', 'True', 'True', 'True', 'True', 'False', 'False', 'True', 'True', 'True', 'False', 'False', 'True', 'False', 'True', 'True', 'False', 'False', 'True', 'True', 'True', 'False', 'True', 'False', 'False', 'True', 'True', 'False', 'False', 'False', 'True', 'False', 'True', 'True', 'True', 'False', 'False', 'True', 'False', 'True', 'True', 'True', 'True', 'True', 'True', 'False', 'True', 'False', 'True', 'False', 'True', 'False', 'True', 'True', 'False', 'True', 'True', 'True', 'True', 'False', 'True', 'True', 'True', 'False', 'True', 'False', 'True', 'False', 'True', 'True', 'True', 'False', 'False', 'True', 'False', 'True', 'False', 'True', 'True', 'False', 'True', 'True', 'False', 'True', 'False', 'False', 'True', 'True', 'True', 'False', 'True', 'True', 'True', 'False', 'True', 'False', 'True', 'True', 'True', 'False', 'True', 'True', 'True', 'False', 'True', 'True', 'True', 'False', 'True', 'False', 'True', 'True', 'False', 'False', 'False', 'False', 'True', 'True', 'True', 'True', 'True', 'False', 'False', 'True', 'True', 'True', 'True', 'False', 'True', 'True', 'False', 'False', 'True', 'False', 'False', 'False', 'False', 'True', 'True', 'True', 'False', 'False', 'False', 'False', 'True', 'True', 'False', 'False', 'True', 'True', 'False', 'False', 'False', 'True', 'False', 'False', 'True', 'False', 'True', 'False', 'True', 'True', 'False', 'False', 'False', 'True', 'False', 'False', 'True', 'True', 'True', 'False', 'False', 'True', 'True', 'False', 'True', 'True', 'True', 'True', 'True', 'False', 'True', 'True', 'True', 'True', 'True', 'False', 'False', 'False', 'True', 'True', 'False', 'True', 'True', 'True', 'True', 'True', 'False', 'True', 'False', 'True', 'False', 'True', 'False', 'True', 'False', 'False', 'True', 'True', 'False', 'False', 'True', 'True', 'False', 'False', 'False', 'True', 'True', 'True', 'True', 'True', 'False', 'False', 'True', 'True', 'True', 'True', 'True', 'False', 'True', 'False', 'True', 'False', 'False', 'True', 'True', 'True', 'True', 'False', 'False', 'True', 'False', 'False', 'True', 'True', 'True', 'False', 'True', 'True', 'False', 'False', 'True', 'False', 'False', 'True', 'False', 'True', 'True', 'True', 'False', 'False', 'False', 'False', 'True', 'False', 'True', 'False', 'True', 'False', 'False', 'True', 'True', 'True', 'False', 'False', 'True', 'True', 'False', 'True', 'True', 'True', 'True', 'True', 'True', 'True', 'False', 'True', 'True', 'True', 'False', 'True', 'False', 'False', 'True', 'True', 'True', 'True', 'True', 'False', 'True', 'False', 'True', 'True', 'False', 'False', 'True', 'False', 'False', 'False', 'True', 'True', 'True', 'True', 'True', 'False', 'True', 'True', 'False', 'True', 'True', 'False', 'False', 'True', 'True', 'True', 'True', 'True', 'False', 'False', 'True', 'False', 'False', 'True', 'True', 'True', 'False', 'True', 'True', 'False', 'True', 'False', 'False', 'False', 'True', 'True', 'False', 'True', 'True', 'True', 'True', 'True', 'True', 'True', 'False', 'False', 'True', 'False', 'False', 'False', 'False', 'True', 'True', 'True', 'True', 'True', 'True', 'True', 'True', 'False', 'True', 'False', 'True', 'False', 'True', 'True', 'True', 'True', 'False', 'False', 'False', 'False', 'False', 'True', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'True', 'True', 'False', 'False', 'True', 'True', 'True', 'True', 'True', 'True', 'True', 'False', 'False', 'False', 'False', 'True', 'False', 'True', 'True', 'False', 'True', 'True', 'True', 'False', 'False', 'False', 'False', 'True', 'True', 'True', 'True', 'False', 'True', 'True', 'True', 'False', 'False', 'False', 'False', 'True', 'True', 'True', 'False', 'False', 'False', 'False', 'True', 'True', 'False', 'False', 'False', 'True', 'False', 'False', 'True', 'False', 'True', 'False', 'True', 'True', 'True', 'True', 'True', 'True', 'True', 'True', 'False', 'True', 'True', 'True', 'True', 'False', 'True', 'False', 'True', 'True', 'False', 'False', 'True', 'True', 'True', 'False', 'False', 'True', 'False', 'True', 'True', 'False', 'False', 'False', 'True', 'True', 'True', 'True', 'True', 'True', 'True', 'False', 'True', 'False', 'False', 'True', 'True', 'False', 'False', 'True', 'True', 'False', 'True', 'False', 'True', 'False', 'True', 'False', 'False', 'False', 'True', 'True', 'True', 'True', 'True', 'True', 'True', 'True', 'False', 'False', 'False', 'False', 'False', 'False', 'True', 'False', 'True', 'False', 'False', 'False', 'True', 'True', 'False', 'False', 'True', 'True', 'True', 'False', 'False', 'True', 'True', 'False', 'False', 'True', 'False', 'False', 'True', 'False', 'True', 'True', 'True', 'True', 'True', 'False', 'True', 'False', 'False', 'True', 'False', 'True', 'False', 'True', 'True', 'True', 'False', 'True', 'False', 'True', 'False', 'True', 'False', 'True', 'True', 'True', 'True', 'False', 'True', 'False', 'False', 'False', 'True', 'True', 'True', 'True', 'True', 'True', 'False', 'False', 'True', 'True', 'False', 'False', 'True', 'False', 'True', 'True', 'True', 'True', 'False', 'False', 'True', 'False', 'True', 'True', 'False', 'True', 'False', 'False', 'True', 'False', 'True', 'True', 'True', 'True', 'True', 'True', 'False', 'False', 'True', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'True', 'False', 'True', 'False', 'False', 'False', 'False', 'True', 'True', 'False', 'True', 'False', 'True', 'True', 'True', 'True', 'True', 'False', 'True', 'True', 'False', 'True', 'False', 'True', 'True', 'False', 'True', 'False', 'True', 'True', 'True', 'True', 'True', 'True', 'False', 'False', 'False', 'False', 'False', 'True', 'True', 'True', 'True', 'True', 'False', 'True', 'True', 'True', 'True', 'True', 'True', 'False', 'True', 'True', 'False', 'True', 'True', 'False', 'True', 'True', 'True', 'True', 'False', 'True', 'False', 'False', 'False', 'True', 'False', 'True', 'True', 'False', 'False', 'False', 'False', 'True', 'True', 'False', 'False', 'False', 'True', 'True', 'False', 'True', 'False', 'True', 'True', 'True', 'True', 'True', 'False', 'False', 'True', 'True', 'True', 'False', 'False', 'False', 'False', 'True', 'False', 'False', 'True', 'True', 'True', 'False', 'False', 'True', 'False', 'False', 'False', 'True', 'False', 'False', 'True', 'False', 'True', 'True', 'True', 'True', 'False', 'True', 'True', 'True', 'True', 'True', 'True', 'False', 'False', 'False', 'True', 'False', 'False', 'True', 'True', 'False', 'False', 'True', 'False', 'False', 'False', 'False', 'True', 'True', 'True', 'True', 'False', 'True', 'True', 'True', 'True', 'True', 'True', 'False', 'False', 'False', 'True', 'False', 'False', 'True', 'False', 'True', 'True', 'False', 'True', 'True', 'False', 'True', 'False', 'False', 'False', 'True', 'False', 'False', 'False', 'False', 'True', 'True', 'False', 'True', 'False', 'False', 'False', 'True', 'True', 'False', 'True', 'True', 'False', 'False', 'True', 'False', 'True', 'True', 'False', 'False', 'True', 'False', 'True', 'False', 'True', 'True', 'False', 'False', 'False', 'True', 'False', 'True', 'False', 'False', 'False', 'True', 'True', 'False', 'True', 'True', 'True', 'True', 'False', 'True', 'True', 'False', 'True', 'False', 'False', 'True', 'False', 'True', 'True', 'False', 'True', 'False', 'False', 'True', 'True', 'False', 'False', 'True', 'False', 'False', 'False', 'True', 'False', 'False', 'True', 'True', 'False', 'False', 'False', 'False', 'True', 'False', 'False', 'True', 'False', 'True', 'True', 'True', 'True', 'False', 'False', 'False', 'True', 'True', 'True', 'True', 'True', 'False', 'True', 'True', 'False', 'True', 'True', 'True', 'True', 'True', 'True', 'True', 'True', 'True', 'False', 'True', 'True', 'True', 'True', 'True', 'True', 'True', 'True', 'True', 'True', 'False', 'False', 'False', 'False', 'False', 'False', 'True', 'False', 'False', 'False', 'True', 'True', 'False', 'False', 'True', 'True', 'True', 'False', 'False', 'False', 'True', 'False', 'True', 'False', 'False', 'True', 'True', 'False', 'True', 'True', 'True', 'False', 'True', 'False', 'False', 'True', 'True', 'True', 'False', 'True', 'True', 'True', 'True', 'False', 'False', 'False', 'True', 'False', 'True', 'False', 'False', 'True', 'False', 'False', 'True', 'True', 'True', 'False', 'False', 'True', 'True', 'False', 'False', 'False', 'True', 'True', 'False', 'False', 'False', 'False', 'False', 'True', 'True', 'False', 'True', 'True', 'True', 'True', 'True', 'True', 'True', 'False', 'True', 'True', 'True', 'True', 'False', 'True', 'True', 'True', 'True', 'False', 'True', 'False', 'True', 'True', 'True', 'True', 'True', 'False', 'True', 'True', 'False', 'True', 'False', 'True', 'False', 'True', 'True', 'True', 'True', 'False', 'False', 'False', 'False', 'False', 'True', 'False', 'True', 'True', 'False', 'False', 'True', 'True', 'True', 'True', 'True', 'False', 'False', 'False', 'True', 'True', 'False', 'True', 'False', 'True', 'False', 'False', 'True', 'False', 'False', 'False', 'False', 'True', 'False', 'False', 'False', 'True', 'True', 'False', 'False', 'False', 'True', 'True', 'True', 'False', 'False', 'False', 'False', 'False', 'False', 'True', 'True', 'False', 'False', 'False', 'True', 'False', 'False', 'True', 'False', 'False', 'False', 'True', 'False', 'True', 'False', 'True', 'True', 'True', 'True', 'True', 'True', 'False', 'True', 'True', 'True', 'True', 'True', 'True', 'False', 'False', 'False', 'False', 'False', 'False', 'True', 'False', 'False', 'True', 'False', 'True', 'True', 'True', 'True', 'True', 'False', 'False', 'True', 'False', 'True', 'True', 'True', 'True', 'False', 'False', 'True', 'False', 'True', 'True', 'True', 'True', 'True', 'True', 'False', 'False', 'False', 'False', 'True', 'True', 'False', 'True', 'False', 'True', 'False', 'False', 'True', 'False', 'True', 'True', 'False', 'True', 'True', 'True', 'True', 'False', 'False', 'False', 'False', 'True', 'True', 'True', 'False', 'True', 'False', 'True', 'False', 'False', 'True', 'False', 'False', 'True', 'False', 'False', 'False', 'True', 'False', 'True', 'False', 'True', 'True', 'False', 'False', 'True', 'True', 'True', 'True', 'False', 'True', 'False', 'False', 'False', 'False', 'True', 'False', 'True', 'False', 'False', 'True', 'True', 'True', 'False', 'True', 'False', 'True', 'True', 'False', 'True', 'False', 'True', 'True', 'False', 'True', 'False', 'False', 'False', 'True', 'True', 'False', 'False', 'True', 'True', 'True', 'False', 'False', 'True', 'False', 'True', 'True', 'True', 'False', 'True', 'False', 'True', 'False', 'True', 'True', 'False', 'True', 'False', 'False', 'False', 'False', 'True', 'True', 'True', 'True', 'False', 'True', 'True', 'False', 'False', 'False', 'True', 'False', 'False', 'True', 'False', 'True', 'True', 'True', 'True', 'False', 'False', 'True', 'False', 'False', 'False', 'False', 'False', 'False', 'True', 'True', 'False', 'False', 'False', 'True', 'True', 'False', 'True', 'True', 'True', 'False', 'False', 'True', 'False', 'True', 'False', 'False', 'True', 'False', 'True', 'False', 'False', 'False', 'True', 'True', 'False', 'True', 'False', 'False', 'False', 'True', 'False', 'True', 'False', 'True', 'True', 'False', 'True', 'True', 'True', 'False', 'True', 'False', 'True', 'True', 'False', 'False', 'True', 'True', 'False', 'False', 'False', 'True', 'False', 'False', 'True', 'False', 'False', 'False', 'True', 'True', 'False', 'True', 'True', 'False', 'False', 'True', 'True', 'False', 'True', 'False', 'True', 'True', 'True', 'False', 'True', 'True', 'False', 'True', 'True', 'False', 'True', 'True', 'True', 'True', 'False', 'True', 'True', 'True', 'True', 'False', 'False', 'True', 'False', 'True', 'False', 'True', 'False', 'False', 'False', 'True', 'True', 'False', 'True', 'True', 'False', 'False', 'False', 'True', 'False', 'True', 'False', 'True', 'True', 'False', 'True', 'True', 'False', 'False', 'True', 'True', 'True', 'False', 'True', 'True', 'False', 'True', 'True', 'False', 'True', 'False', 'False', 'True', 'True', 'True', 'True', 'True', 'True', 'True', 'True', 'True', 'True', 'True', 'False', 'True', 'False', 'True', 'False', 'False', 'True', 'False', 'True', 'True', 'True', 'True', 'True', 'True', 'True', 'False', 'True', 'True', 'True', 'True', 'True', 'False', 'False', 'False', 'True', 'False', 'True', 'True', 'True', 'False', 'False', 'False', 'True', 'True', 'False', 'False', 'True', 'True', 'True', 'True', 'True', 'False', 'False', 'False', 'True', 'True', 'False', 'False', 'False', 'True', 'True', 'False', 'True', 'True', 'False', 'False', 'False', 'True', 'False', 'True', 'False', 'True', 'True', 'True', 'False', 'False', 'True', 'False', 'True', 'True', 'True', 'True', 'True', 'False', 'False', 'True', 'True', 'False', 'False', 'True', 'True', 'True', 'True', 'True', 'False', 'False', 'True', 'False', 'True', 'False', 'False', 'True', 'True', 'False', 'True', 'True', 'False', 'False', 'True', 'False', 'False', 'True', 'True', 'True', 'False', 'False', 'False', 'True', 'True', 'False', 'False', 'False', 'False', 'False', 'True', 'True', 'False', 'True', 'True', 'False', 'False', 'False', 'True', 'False', 'True', 'False', 'False', 'True', 'False', 'True', 'False', 'False', 'False', 'True', 'True', 'True', 'True', 'True', 'False', 'False', 'False', 'True', 'True', 'True', 'True', 'False', 'True', 'True', 'True', 'True', 'True', 'True', 'True', 'True', 'True', 'True', 'True', 'True', 'False', 'False', 'False', 'True', 'False', 'False', 'False', 'True', 'True', 'True', 'False', 'True', 'False', 'False', 'True', 'False', 'True', 'True', 'True', 'True', 'True', 'False', 'False', 'True', 'True', 'True', 'False', 'False', 'False', 'False', 'True', 'False', 'True', 'False', 'True', 'True', 'True', 'False', 'False', 'False', 'True', 'True', 'False', 'True', 'False', 'True', 'True', 'True', 'True', 'False', 'False', 'True', 'False', 'True', 'False', 'True', 'False', 'False', 'True', 'False', 'False', 'False', 'False', 'True', 'True', 'False', 'True', 'False', 'False', 'False', 'False', 'True', 'True', 'True', 'True', 'True', 'False', 'False', 'False', 'True', 'True', 'True', 'True', 'True', 'True', 'True', 'True', 'False', 'False', 'False', 'False', 'False', 'True', 'True', 'True', 'False', 'False', 'True', 'False', 'True', 'True', 'True', 'False', 'False', 'False', 'True', 'True', 'False', 'False', 'True', 'True', 'False', 'True', 'True', 'True', 'True', 'True', 'False', 'False', 'True', 'True', 'False', 'True', 'True', 'True', 'True', 'True', 'False', 'True', 'True', 'False', 'True', 'True', 'False', 'True', 'False', 'False', 'True', 'False', 'True', 'False', 'True', 'False', 'True', 'True', 'False', 'False', 'False', 'False', 'False', 'True', 'True', 'False', 'False', 'False', 'False', 'True', 'False', 'False', 'True', 'False', 'False', 'True', 'True', 'True', 'True', 'True', 'True', 'False', 'True', 'False', 'False', 'True', 'True', 'True', 'True', 'False', 'False', 'True', 'False', 'False', 'False', 'False', 'False', 'True', 'False', 'True', 'False', 'False', 'True', 'False', 'True', 'True', 'False', 'True', 'True', 'True', 'False', 'False', 'False', 'True', 'False', 'True', 'False', 'False', 'True', 'True', 'True', 'True', 'True', 'False', 'False', 'False', 'False', 'True', 'True', 'True', 'True', 'True', 'True', 'True', 'True', 'True', 'True', 'False', 'False', 'False', 'False', 'False', 'False', 'True', 'True', 'True', 'False', 'False', 'False', 'False', 'False', 'True', 'True', 'False', 'False', 'False', 'True', 'True', 'True', 'False', 'False', 'True', 'True', 'True', 'True', 'False', 'True', 'False', 'True', 'True', 'True', 'True', 'False', 'False', 'False', 'False', 'True', 'True', 'False', 'True', 'False', 'False', 'False', 'True', 'True', 'True', 'True', 'True', 'True', 'True', 'True', 'False', 'True', 'True', 'True', 'False', 'False', 'False', 'True', 'False', 'True', 'False', 'True', 'False', 'True', 'False', 'True', 'False', 'False', 'True', 'False', 'True', 'False', 'True', 'True', 'False', 'False', 'False', 'False', 'True', 'True', 'False', 'True', 'True', 'True', 'False', 'True', 'True', 'True', 'True', 'True', 'True', 'False', 'False', 'True', 'False', 'False', 'False', 'False', 'True', 'False', 'True', 'True', 'False', 'True', 'True', 'False', 'True', 'False', 'False', 'True', 'True', 'True', 'False', 'False', 'True', 'False', 'False', 'False', 'False', 'False', 'True', 'True', 'True', 'True', 'True', 'False', 'True', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'True', 'True', 'False', 'True', 'False', 'False', 'False', 'False', 'True', 'False', 'False', 'True', 'False', 'True', 'True', 'False', 'False', 'False', 'True', 'False', 'True', 'False', 'True', 'True', 'False', 'True', 'True', 'True', 'False', 'True', 'False', 'True', 'True', 'False', 'True', 'False', 'False', 'True', 'False', 'True', 'True', 'False', 'True', 'True', 'True', 'False', 'True', 'True', 'True', 'False', 'True', 'True', 'True', 'True', 'True', 'False', 'True', 'False', 'False', 'True', 'False', 'True', 'False', 'True', 'True', 'True', 'True', 'True', 'True', 'False', 'False', 'False', 'True', 'True', 'True', 'True', 'False', 'True', 'False', 'False', 'True', 'False', 'True', 'True', 'True', 'True', 'True', 'False', 'False', 'False', 'False', 'True', 'True', 'False', 'True', 'True', 'False', 'False', 'True', 'False', 'True', 'False', 'True', 'False', 'False', 'False', 'True', 'True', 'False', 'False', 'False', 'True', 'True', 'True', 'False', 'True', 'True', 'True', 'False', 'True', 'False', 'True', 'True', 'False', 'False', 'False', 'False', 'True', 'True', 'False', 'False', 'False', 'True', 'True', 'False', 'False', 'True', 'False', 'True', 'False', 'True', 'False', 'False', 'False', 'False', 'True', 'True', 'True', 'False', 'False', 'False', 'False', 'True', 'True', 'True', 'True', 'False', 'True', 'True', 'False', 'True', 'False', 'False', 'False', 'False', 'False', 'True', 'False', 'True', 'True', 'True', 'False', 'True', 'True', 'False', 'True', 'True', 'True', 'False', 'False', 'True', 'True', 'False', 'True', 'True', 'True', 'False', 'True', 'True', 'True', 'True', 'True', 'False', 'True', 'False', 'True', 'False', 'True', 'True', 'False', 'True', 'True', 'True', 'True', 'False', 'False', 'False', 'True', 'True', 'False', 'True', 'False', 'True', 'False', 'True', 'True', 'False', 'True', 'True', 'True', 'False', 'True', 'True', 'True', 'False', 'False', 'True', 'False', 'True', 'False', 'False', 'True', 'False', 'False', 'False', 'True', 'False', 'True', 'False', 'True', 'True', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'True', 'False', 'True', 'True', 'False', 'True', 'True', 'True', 'True', 'True', 'True', 'False', 'False', 'True', 'False', 'True', 'True', 'False', 'True', 'False', 'False', 'True', 'False', 'True', 'False', 'False', 'False', 'True', 'True', 'True', 'True', 'True', 'True', 'False', 'True', 'True', 'False', 'False', 'False', 'True', 'False', 'True', 'False', 'True', 'True', 'True', 'True', 'True', 'False', 'False', 'True', 'True', 'True', 'True', 'True', 'False', 'True', 'True', 'False', 'False', 'True', 'True', 'False', 'True', 'False', 'False', 'True', 'True', 'True', 'False', 'True', 'False', 'False', 'True', 'True', 'True', 'False', 'False', 'False', 'False', 'False', 'True', 'True', 'True', 'True', 'False', 'False', 'True', 'True', 'True', 'True', 'True', 'True', 'True', 'True', 'True', 'False', 'False', 'True', 'True', 'False', 'False', 'False', 'True', 'True', 'True', 'True', 'False', 'False', 'True', 'True', 'False', 'False', 'False', 'True', 'False', 'False', 'False', 'False', 'True', 'False', 'False', 'True', 'True', 'False', 'True', 'False', 'False', 'True', 'True', 'True', 'False', 'True', 'False', 'True', 'True', 'True', 'False', 'False', 'True', 'False', 'False', 'False', 'False', 'False', 'False', 'True', 'False', 'True', 'True', 'True', 'True', 'True', 'True', 'True', 'False', 'True', 'False', 'False', 'False', 'False', 'True', 'True', 'True', 'False', 'True', 'False', 'True', 'True', 'False', 'False', 'False', 'True', 'False', 'True', 'False', 'True', 'False', 'False', 'True', 'False', 'False', 'True', 'True', 'True', 'True', 'True', 'True', 'True', 'True', 'False', 'False', 'True', 'True', 'False', 'True', 'False', 'True', 'False', 'True', 'True', 'False', 'False', 'False', 'True', 'True', 'False', 'True', 'True', 'True', 'True', 'False', 'True', 'True', 'False', 'True', 'True', 'True', 'True', 'True', 'True', 'True', 'False', 'True', 'False', 'False', 'False', 'True', 'False', 'True', 'False', 'True', 'False', 'True', 'True', 'True', 'False', 'False', 'False', 'False', 'True', 'False', 'True', 'False', 'True', 'True', 'True', 'True', 'True', 'True', 'False', 'True', 'True', 'False', 'True', 'True', 'True', 'True', 'False', 'True', 'True', 'False', 'False', 'True', 'True', 'True', 'False', 'False', 'False', 'True', 'True', 'False', 'True', 'True', 'False', 'False', 'True', 'True', 'False', 'True', 'True', 'True', 'True', 'True', 'False', 'False', 'False', 'True', 'True', 'True', 'True', 'True', 'False', 'False', 'False', 'True', 'True', 'False', 'True', 'False', 'True', 'False', 'True', 'True', 'True', 'True', 'True', 'False', 'True', 'False', 'True', 'True', 'False', 'False', 'False', 'True', 'True', 'True', 'True', 'False', 'True', 'False', 'True', 'True', 'False', 'True', 'True', 'False', 'False', 'False', 'True', 'True', 'False', 'False', 'True', 'True', 'True', 'True', 'True', 'False', 'True', 'False', 'True', 'True', 'True', 'True', 'True', 'False', 'True', 'False', 'False', 'True', 'False', 'True', 'True', 'False', 'True', 'True', 'False', 'True', 'False', 'True', 'False', 'False', 'True', 'True', 'False', 'False', 'False', 'True', 'True', 'True', 'False', 'False', 'True', 'True', 'False', 'False', 'False', 'True', 'True', 'False', 'True', 'True', 'True', 'True', 'True', 'True', 'True', 'False', 'False', 'True', 'False', 'True', 'False', 'False', 'True', 'True', 'False', 'False', 'True', 'False', 'False', 'True', 'True', 'True', 'True', 'True', 'True', 'False', 'False', 'False', 'False', 'True', 'False', 'False', 'False', 'True', 'False', 'True', 'True', 'False', 'False', 'True', 'False', 'False', 'True', 'True', 'False', 'False', 'True', 'False', 'False', 'True', 'True', 'True', 'False', 'True', 'False', 'False', 'True', 'False', 'True', 'False', 'True', 'True', 'True', 'False', 'True', 'True', 'False', 'True', 'True', 'True', 'True', 'True', 'False', 'False', 'False', 'True', 'True', 'True', 'True', 'True', 'False', 'False', 'False', 'True', 'True', 'False', 'True', 'False', 'False', 'False', 'False', 'False', 'False', 'True', 'False', 'True', 'True', 'True', 'True', 'True', 'False', 'True', 'True', 'False', 'False', 'True', 'True', 'True', 'False', 'True', 'True', 'False', 'True', 'False', 'True', 'True', 'True', 'False', 'False', 'True', 'False', 'True', 'True', 'False', 'True', 'True', 'False', 'False', 'True', 'True', 'True', 'True', 'True', 'True', 'True', 'False', 'True', 'True', 'False', 'True', 'True', 'False', 'True', 'False', 'True', 'True', 'True', 'True', 'False', 'True', 'False', 'True', 'False', 'True', 'False', 'True', 'False', 'False', 'False', 'False', 'True', 'False', 'True', 'True', 'False', 'True', 'True', 'False', 'True', 'False', 'True', 'False', 'True', 'False', 'False', 'False', 'False', 'True', 'False', 'True', 'False', 'True', 'False', 'False', 'False', 'True', 'False', 'True', 'False', 'False', 'False', 'False', 'True', 'False', 'False', 'False', 'True', 'True', 'True', 'True', 'False', 'True', 'False', 'True', 'True', 'True', 'True', 'False', 'True', 'True', 'True', 'False', 'False', 'False', 'True', 'True', 'True', 'False', 'True', 'False', 'False', 'False', 'False', 'True', 'True', 'True', 'True', 'True', 'True', 'False', 'False', 'True', 'False', 'False', 'False', 'True', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'True', 'True', 'False', 'False', 'True', 'True', 'True', 'True', 'False', 'True', 'False', 'True', 'True', 'False', 'True', 'True', 'False', 'False', 'False', 'True', 'True', 'False', 'True', 'True', 'True', 'True', 'True', 'True', 'False', 'True', 'True', 'False', 'False', 'False', 'True', 'True', 'True', 'True', 'True', 'True', 'False', 'False', 'False', 'False', 'False', 'True', 'True', 'False', 'False', 'True', 'False', 'False', 'False', 'True', 'False', 'True', 'False', 'True', 'True', 'True', 'True', 'True', 'True', 'True', 'False', 'True', 'False', 'False', 'False', 'True', 'False', 'True', 'False', 'False', 'True', 'True', 'False', 'False', 'False', 'True', 'True', 'True', 'False', 'False', 'True', 'True', 'False', 'True', 'True', 'True', 'False', 'True', 'False', 'True', 'True', 'True', 'True', 'False', 'False', 'False', 'True', 'False', 'True', 'True', 'False', 'True', 'True', 'True', 'False', 'True', 'True', 'False', 'True', 'False', 'True', 'True', 'False', 'False', 'True', 'True', 'False', 'True', 'True', 'True', 'True', 'True', 'True', 'False', 'False', 'False', 'False', 'False', 'True', 'False', 'True', 'False', 'False', 'False', 'True', 'True', 'False', 'False', 'True', 'False', 'False', 'False', 'True', 'True', 'True', 'True', 'True', 'True', 'False', 'False', 'True', 'True', 'False', 'True', 'True', 'False', 'True', 'True', 'True', 'False', 'True', 'True', 'True', 'False', 'True', 'True', 'True', 'False', 'True', 'True', 'True', 'False', 'True', 'True', 'False', 'False', 'False', 'False', 'False', 'False', 'True', 'True', 'False', 'True', 'True', 'True', 'True', 'True', 'False', 'True', 'False', 'True', 'False', 'False', 'True', 'False', 'True', 'True', 'True', 'True', 'True', 'False', 'False', 'False', 'False', 'True', 'False', 'False', 'False', 'True', 'False', 'True', 'False', 'False', 'True', 'True', 'False', 'False', 'True', 'True', 'True', 'False', 'True', 'False', 'False', 'True', 'False', 'True', 'True', 'True', 'False', 'True', 'True', 'True', 'False', 'True', 'True', 'False', 'True', 'True', 'True', 'True', 'False', 'False', 'True', 'True', 'True', 'False', 'True', 'False', 'False', 'True', 'True', 'True', 'True', 'False', 'False', 'False', 'False', 'False', 'True', 'True', 'False', 'True', 'False', 'True', 'True', 'True', 'True', 'False', 'False', 'True', 'False', 'True', 'True', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'True', 'True', 'False', 'False', 'False', 'True', 'True', 'False', 'True', 'False', 'False', 'False', 'False', 'True', 'True', 'True', 'False', 'True', 'False', 'True', 'True', 'True', 'False', 'True', 'True', 'False', 'False', 'False', 'True', 'True', 'True', 'True', 'False', 'True', 'True', 'True', 'True', 'True', 'False', 'True', 'True', 'False', 'True', 'True', 'False', 'True', 'False', 'True', 'True', 'False', 'True', 'False', 'False', 'False', 'True', 'False', 'False', 'False', 'False', 'True', 'True', 'True', 'False', 'True', 'True', 'True', 'True', 'True', 'True', 'True', 'True', 'True', 'False', 'True', 'False', 'True', 'False', 'True', 'False', 'True', 'True', 'False', 'True', 'True', 'False', 'False', 'False', 'True', 'True', 'False', 'False', 'True', 'True', 'False', 'True', 'False', 'True', 'False', 'True', 'True', 'True', 'True', 'True', 'True', 'False', 'True', 'True', 'False', 'True', 'False', 'True', 'True', 'False', 'False', 'False', 'False', 'False', 'False', 'True', 'True', 'True', 'False', 'False', 'True', 'True', 'True', 'False', 'True', 'True', 'False', 'False', 'False', 'True', 'False', 'True', 'True', 'True', 'True', 'False', 'True', 'False', 'True', 'False', 'True', 'True', 'True', 'False', 'True', 'True', 'False', 'True', 'True', 'True', 'False', 'True', 'False', 'False', 'True', 'False', 'False', 'True', 'True', 'False', 'True', 'True', 'False', 'False', 'True', 'False', 'True', 'False', 'True', 'False', 'True', 'True', 'False', 'True', 'False', 'False', 'True', 'True', 'False', 'True', 'False', 'True', 'False', 'False', 'False', 'True', 'True', 'True', 'False', 'True', 'True', 'True', 'True', 'True', 'True', 'False', 'False', 'False', 'False', 'False', 'False', 'True', 'False', 'True', 'False', 'True', 'True', 'True', 'False', 'False', 'True', 'True', 'True', 'True', 'False', 'False', 'True', 'True', 'True', 'True', 'True', 'False', 'False', 'True', 'False', 'True', 'False', 'False', 'True', 'False', 'True', 'False', 'True', 'True', 'True', 'False', 'True', 'True', 'False', 'True', 'True', 'True', 'False', 'False', 'False', 'False', 'False', 'True', 'True', 'False', 'True', 'True', 'True', 'True', 'True', 'False', 'False', 'False', 'True', 'True', 'True', 'False', 'True', 'False', 'False', 'True', 'True', 'True', 'False', 'True', 'True', 'False', 'True', 'True', 'True', 'True', 'True', 'True', 'False', 'False', 'False', 'True', 'False', 'True', 'True', 'False', 'False', 'True', 'True', 'True', 'True', 'True', 'False', 'False', 'True', 'True', 'True', 'True', 'False', 'True', 'True', 'True', 'False', 'False', 'False', 'True', 'True', 'True', 'True', 'False', 'True', 'True', 'False', 'True', 'False', 'False', 'True', 'True', 'False', 'True', 'False', 'False', 'False', 'True', 'False', 'False', 'False', 'False', 'False', 'True', 'True', 'False', 'True', 'True', 'True', 'False', 'True', 'True', 'False', 'False', 'False', 'True', 'True', 'True', 'True', 'True', 'True', 'False', 'True', 'False', 'False', 'False', 'False', 'False', 'True', 'True', 'True', 'True', 'True', 'True', 'False', 'True', 'True', 'False', 'True', 'False', 'False', 'True', 'False', 'True', 'True', 'True', 'False', 'False', 'False', 'True', 'True', 'False', 'False', 'True', 'False', 'False', 'True', 'False', 'True', 'False', 'False', 'False', 'False', 'False', 'True', 'True', 'True', 'False', 'False', 'False', 'False', 'True', 'True', 'True', 'True', 'True', 'True', 'False', 'True', 'True', 'False', 'True', 'True', 'True', 'True', 'False', 'False', 'True', 'True', 'False', 'True', 'True', 'True', 'False', 'True', 'True', 'True', 'False', 'True', 'False', 'False', 'True', 'True', 'True', 'False', 'True', 'True', 'False', 'False', 'True', 'False', 'True', 'True', 'True', 'False', 'True', 'True', 'True', 'False', 'True', 'False', 'True', 'True', 'False', 'False', 'False', 'True', 'True', 'False', 'True', 'False', 'True', 'True', 'False', 'False', 'False', 'True', 'False', 'True', 'True', 'False', 'False', 'False', 'True', 'False', 'False', 'False', 'True', 'True', 'True', 'False', 'True', 'True', 'True', 'False', 'True', 'True', 'False', 'True', 'False', 'True', 'False', 'True', 'True', 'False', 'False', 'True', 'True', 'True', 'False', 'False', 'False', 'True', 'False', 'True', 'False', 'False', 'False', 'True', 'True', 'True', 'True', 'False', 'False', 'False', 'True', 'True', 'False', 'False', 'False', 'False', 'True', 'True', 'True', 'True', 'False', 'True', 'True', 'True', 'True', 'False', 'False', 'False', 'True', 'False', 'True', 'True', 'False', 'False', 'False', 'False', 'True', 'True', 'True', 'True', 'False', 'True', 'False', 'False', 'False', 'False', 'True', 'True', 'True', 'True', 'True', 'False', 'True', 'True', 'True', 'False', 'True', 'False', 'True', 'True', 'False', 'True', 'True', 'False', 'True', 'True', 'True', 'True', 'False', 'True', 'False', 'False', 'False', 'False', 'True', 'True', 'False', 'False', 'True', 'True', 'False', 'False', 'True', 'True', 'False', 'False', 'False', 'False', 'True', 'True', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'True', 'False', 'False', 'False', 'False', 'True', 'False', 'True', 'False', 'True', 'True', 'False', 'False', 'False', 'False', 'True', 'True', 'False', 'True', 'False', 'True', 'True', 'True', 'True', 'True', 'True', 'True', 'True', 'True', 'False', 'False', 'False', 'True', 'True', 'False', 'False', 'True', 'False', 'False', 'False', 'True', 'True', 'False', 'True', 'False', 'True', 'True', 'False', 'True', 'False', 'True', 'True', 'False', 'True', 'True', 'True', 'True', 'False', 'True', 'False', 'True', 'True', 'True', 'False', 'False', 'True', 'False', 'True', 'True', 'True', 'True', 'True', 'False', 'False', 'True', 'True', 'True', 'True', 'True', 'True', 'True', 'True', 'True', 'False', 'True', 'False', 'False', 'True', 'False', 'True', 'False', 'False', 'True', 'False', 'False', 'False', 'False', 'False', 'False', 'True', 'True', 'False', 'True', 'False', 'True', 'False', 'True', 'True', 'False', 'False', 'False', 'True', 'False', 'False', 'True', 'True', 'False', 'True', 'False', 'False', 'True', 'False', 'False', 'False', 'True', 'False', 'False', 'False', 'True', 'False', 'True', 'False', 'True', 'True', 'False', 'True', 'False', 'True', 'True', 'False', 'False', 'False', 'False', 'True', 'True', 'False', 'True', 'True', 'True', 'False', 'False', 'False', 'False', 'True', 'True', 'True', 'False', 'True', 'False', 'True', 'False', 'True', 'True', 'True', 'False', 'False', 'True', 'False', 'True', 'False', 'False', 'False', 'True', 'True', 'False', 'True', 'True', 'True', 'True', 'False', 'True', 'True', 'True', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'True', 'True', 'False', 'True', 'False', 'False', 'True', 'True', 'True', 'False', 'False', 'True', 'True', 'True', 'True', 'False', 'True', 'False', 'True', 'False', 'False', 'False', 'True', 'False', 'True', 'True', 'False', 'False', 'False', 'True', 'False', 'True', 'False', 'True', 'True', 'False', 'False', 'False', 'True', 'True', 'True', 'True', 'False', 'True', 'True', 'False', 'False', 'True', 'False', 'True', 'True', 'True', 'True', 'True', 'True', 'True', 'True', 'True', 'True', 'True', 'True', 'True']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "score_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "iTs1s1RN_I-2",
        "outputId": "6fcea1ba-c053-4c84-dcc8-b677179fde8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          0\n",
              "0      True\n",
              "1     False\n",
              "2      True\n",
              "3      True\n",
              "4      True\n",
              "...     ...\n",
              "4272   True\n",
              "4273   True\n",
              "4274   True\n",
              "4275   True\n",
              "4276   True\n",
              "\n",
              "[4277 rows x 1 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f5a50d69-d00a-4700-912b-2a1d40ae1a60\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4272</th>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4273</th>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4274</th>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4275</th>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4276</th>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4277 rows  1 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f5a50d69-d00a-4700-912b-2a1d40ae1a60')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-f5a50d69-d00a-4700-912b-2a1d40ae1a60 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-f5a50d69-d00a-4700-912b-2a1d40ae1a60');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Export"
      ],
      "metadata": {
        "id": "YRf3Fh0BaM0Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_export = pd.DataFrame({\"PassengerId\":df_test_original[\"PassengerId\"], \"Transported\":var_score})\n",
        "df_export.to_csv('results.csv', index=False)"
      ],
      "metadata": {
        "id": "yQ-qRTlqV8mC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}