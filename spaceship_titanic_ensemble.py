# -*- coding: utf-8 -*-
"""spaceship_titanic_ensemble.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lEsHZ9OaHpVvQtgtsy0Y1DLOfFwdUSyA
"""

from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
import numpy as np # linear algebra
import pandas as pd 

# %matplotlib inline
import matplotlib.pyplot as plt

"""# Read data"""

import pathlib
data_dir = pathlib.Path('/content/drive/MyDrive/ColabNotebooks/spaceship_titanic')

df_train = pd.read_csv(data_dir / "train.csv")
df_test = pd.read_csv(data_dir / "test.csv")
df_test_original = pd.read_csv(data_dir / "test.csv")

#print(df_train.describe())

"""# Check Nulls"""

total = df_train.isnull().sum().sort_values(ascending = False)
percent = (df_train.isnull().sum()/df_train.isnull().count()*100).sort_values(ascending = False)
missing_data  = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])
print(missing_data)

"""# Add Variables"""

col_to_sum = ['RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']

#Create variable SumSpends to gather the sum of spent value by each passenger
df_train['SumSpends'] = df_train[col_to_sum].sum(axis=1)
df_test['SumSpends'] = df_test[col_to_sum].sum(axis=1)


#Cabin column - split into 3 columns, deck, num and side
df_train[['deck','num', 'side']] = df_train['Cabin'].str.split('/', 3, expand=True)
df_test[['deck','num', 'side']] = df_test['Cabin'].str.split('/', 3, expand=True)

df_train.drop('Cabin', axis=1, inplace=True)
df_test.drop('Cabin', axis=1, inplace=True)

#encode categorical variables as integer to improve model performance
from sklearn.preprocessing import OrdinalEncoder
oc = OrdinalEncoder()

size_train = len(df_train)

#join train and test datasets for encoding
df_for_encode = pd.concat([df_train, df_test])

#get columns to be encoded
object_cols = [col for col in df_train.columns if df_train[col].dtype == 'object' or df_train[col].dtype == 'category']
object_cols.append('Transported')

#convert to category to save space
df_for_encode[object_cols] = df_for_encode[object_cols].astype('category')
#encode values
df_for_encode[object_cols] = oc.fit_transform(df_for_encode[object_cols])

del df_train, df_test

#split train and test datasets
df_train = df_for_encode.iloc[:size_train, :]
df_test = df_for_encode.iloc[size_train: , :]
df_test.drop('Transported', axis=1, inplace=True)

del df_for_encode

"""# New Data Cleaning"""

from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer

object_cols = [col for col in df_train.columns if df_train[col].dtype == 'object' or df_train[col].dtype == 'category']

df_train[object_cols] = df_train[object_cols].astype('category')
df_test[object_cols] = df_test[object_cols].astype('category')

null_cols = df_train.isnull().sum().sort_values(ascending=False)
null_cols = list(null_cols[null_cols>1].index)


#Replace null values using sklearn SimpleImputer, different strategies are used for different columns
toMedian = ['num']
toMode = ['CryoSleep','VIP','HomePlanet','Name','deck','side']
toMean = ['Destination','Age','SumSpends','RoomService','FoodCourt','ShoppingMall','Spa','VRDeck']

ct = ColumnTransformer([("imp", SimpleImputer(strategy='mean'), toMean)])
df_train[toMean] = ct.fit_transform(df_train[toMean])
df_test[toMean] = ct.fit_transform(df_test[toMean])


ct2 = ColumnTransformer([("imp", SimpleImputer(strategy='median'), toMedian)])
df_train[toMedian] = ct2.fit_transform(df_train[toMedian])
df_test[toMedian] = ct2.fit_transform(df_test[toMedian])


ct3 = ColumnTransformer([("imp", SimpleImputer(strategy='most_frequent'), toMode)])
df_train[toMode] = ct3.fit_transform(df_train[toMode])
df_test[toMode] = ct3.fit_transform(df_test[toMode])

df_train.head(35)

"""# Delete Outliers"""

import seaborn as sns 
from matplotlib.pyplot import figure

clean_outliers = False
print_boxplot = False

cols_with_outliers = ['RoomService','FoodCourt','ShoppingMall','Spa','VRDeck']
threshold_outliers = [10000, 25000, 10000, 15000, 18000]

if clean_outliers:
  for index, var in enumerate(cols_with_outliers):
    print(df_train[var].shape)
    df_train.drop(df_train[df_train[var] > threshold_outliers[index]].index, axis=0, inplace=True)
    print(df_train[var].shape)

if print_boxplot:
  for index, var in enumerate(df_train.columns):
    figure(figsize=(8, 6), dpi=80)
    sns.boxplot(data=df_train[var], orient='h')
    plt.xlabel(var)
    plt.xlim((0,25000))
    plt.show()
    plt.clf()

"""# Log Transform"""

log_transform = False

if log_transform:
  import seaborn as sns
  cols_logs = ['FoodCourt','ShoppingMall','Spa','VRDeck']
  #teste=df_train.copy(deep=True)

  df_train[cols_logs] = df_train[cols_logs].replace(0, 0.0001)

  for col in cols_logs:
    df_train[col] = np.log(df_train[col])
    #sns.histplot(df_train[col])
    #plt.show()
    #plt.clf()

df_train

"""# Normalization


"""

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()#z-score normalization

df_train2 = df_train.copy(deep=True)#deep = True to create new copy
df_test2 = df_test.copy(deep=True)

#vars_to_normalize = ['RoomService', 'Spa', 'VRDeck', 'SumSpends']
vars_to_normalize = df_test2.columns.to_list()
vars_to_normalize.remove('CryoSleep')
print(vars_to_normalize)

scaler.fit(df_train2[vars_to_normalize])
df_train2[vars_to_normalize] = scaler.transform(df_train2[vars_to_normalize])
df_test2[vars_to_normalize] = scaler.transform(df_test2[vars_to_normalize])

df_train2

df_train2.isnull().sum()

import tensorflow as tf
class MyCallback(tf.keras.callbacks.Callback):
    def on_epoch_end(self, epoch, logs={}):
        if logs.get('val_accuracy') is not None and logs.get('val_accuracy') > 0.81:
            print("Reached 80.8% cal_accuracy so cancelling training!")
            self.model.stop_training = True


callbacks = MyCallback()

"""#Tensorflow - Training"""

import sklearn
from sklearn.model_selection import train_test_split

#vars_to_use_training = ['FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck', 'SumSpends','deck','num','HomePlanet', 'Transported']
vars_to_use_training = ['CryoSleep', 'RoomService', 'Spa', 'VRDeck', 'SumSpends','deck','side','Transported']
df_train2 = df_train2[vars_to_use_training]

#train test split
X_train, X_test, y_train, y_test = train_test_split(df_train2.drop(['Transported'], axis=1), df_train2['Transported'],
                                                    test_size=0.30, random_state=45)

model = tf.keras.Sequential()
#First Hidden Layer
dim = len(df_train2.columns) - 1
model.add(tf.keras.layers.Dense(256, activation='relu', input_dim=dim))
model.add(tf.keras.layers.Dense(128, activation='relu'))

#Second  Hidden Layer
model.add(tf.keras.layers.Dense(4, activation='sigmoid'))
#model.add(tf.keras.layers.Dropout(0.2))

#Output Layer
model.add(tf.keras.layers.Dense(1, activation='sigmoid'))

model.compile(optimizer ='adam',loss='binary_crossentropy', metrics =['accuracy'])

model.fit(X_train, y_train, steps_per_epoch = 80, epochs=25, validation_data=(X_test, y_test), callbacks=[callbacks])

"""# Ensemble"""

install_catboost = True
if install_catboost:
  !pip install catboost
  !pip install ipywidgets
  !pip install lightgbm

from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.naive_bayes import GaussianNB
from catboost import CatBoostClassifier
import lightgbm
from lightgbm import LGBMClassifier
import time
from sklearn.model_selection import GridSearchCV

use_probabilities = False

# Classifiers
classifiers = {
    "LogisticRegression" : LogisticRegression(C=1, penalty='l1', solver='liblinear'),
    "KNN" : KNeighborsClassifier(n_neighbors=9, p=2),
    "SVC" : SVC(C= 1.25, gamma= 'scale', kernel= 'rbf',probability=use_probabilities),
    "RandomForest" : RandomForestClassifier(max_depth= 10, n_estimators= 250),
    "LGBM" : LGBMClassifier(learning_rate= 0.05, max_depth= 8, n_estimators= 100),
    "CatBoost" : CatBoostClassifier(learning_rate= 0.15, max_depth= 4, n_estimators= 100),
    "NaiveBayes": GaussianNB(var_smoothing= 1e-07)
}

# Grids for grid search
LR_grid = {'penalty': ['l1','l2'],
           'C': [0.25, 0.5, 0.75, 1, 1.25, 1.5],
           'max_iter': [50, 100, 150]}

KNN_grid = {'n_neighbors': [3, 5, 7, 9],
            'p': [1, 2]}

SVC_grid = {'C': [0.25, 0.5, 0.75, 1, 1.25, 1.5],
            'kernel': ['linear', 'rbf'],
            'gamma': ['scale', 'auto']}

RF_grid = {'n_estimators': [50, 100, 150, 200, 250, 300],
        'max_depth': [4, 6, 8, 10, 12]}

boosted_grid = {'n_estimators': [50, 100, 150, 200],
        'max_depth': [4, 8, 12],
        'learning_rate': [0.05, 0.1, 0.15]}

NB_grid={'var_smoothing': [1e-10, 1e-9, 1e-8, 1e-7]}

# Dictionary of all grids
grid = {
    "LogisticRegression" : LR_grid,
    "KNN" : KNN_grid,
    "SVC" : SVC_grid,
    "RandomForest" : RF_grid,
    "XGBoost" : boosted_grid,
    "LGBM" : boosted_grid,
    "CatBoost" : boosted_grid,
    "NaiveBayes": NB_grid
}

#Evaluate models using accuracy, models are evaluated on X_test which is the validation dataset
from sklearn.metrics import accuracy_score

i=0
clf_best_params=classifiers.copy()
valid_scores=pd.DataFrame({'Classifer':classifiers.keys(), 'Validation accuracy': np.zeros(len(classifiers)), 'Training time': np.zeros(len(classifiers))})
all_predictions={}
for key, classifier in classifiers.items():
    start = time.time()
    #clf = GridSearchCV(estimator=classifier, param_grid=grid[key], n_jobs=-1, cv=None)
    clf = classifier
    # Train and score
    clf.fit(X_train, y_train)
    print(key)
    
    if use_probabilities:
      y_predictions = clf.predict_proba(X_test)
      y_predictions = [el[1] for el in y_predictions]
    else:
      y_predictions = clf.predict(X_test)


    all_predictions[key] = y_predictions
    var_score = [1 if prediction>0.5 else 0 for prediction in y_predictions ]

    valid_scores.iloc[i,1]=accuracy_score(y_test, var_score)

    
    # Print iteration and training time
    stop = time.time()
    valid_scores.iloc[i,2]=np.round((stop - start)/60, 2)
    
    print('Model:', key)
    print('Training time (mins):', valid_scores.iloc[i,2])
    print('')
    i+=1
print(y_predictions)
print(valid_scores)

valid_scores

all_predictions

#Predictions using ensemble - 5 algorithms
y_predictions_tf = model.predict(X_test)
all_predictions['tf'] = np.array([1 if prediction>0.5 else 0 for prediction in y_predictions_tf ])

#5 models will be used, this problem is of binary classification, so result of prediction is either 0 or 1
#if the sum of the 5 models prediction is 3 or higher it means that a majority of the models predicted that the passengers were transported
best_predictions ={}
best_predictions['RandomForest'] = all_predictions['RandomForest']
best_predictions['LGBM'] = all_predictions['LGBM']
best_predictions['tf'] = all_predictions['tf']
best_predictions['CatBoost'] = all_predictions['CatBoost']
best_predictions['SVC'] = all_predictions['SVC']

sum_predictions = np.zeros(len(best_predictions['tf']))
for pred in best_predictions.values():
  for i, v in enumerate(pred):
    sum_predictions[i] += v


ensemble_predictions = np.array([1 if s>=3 else 0 for s in sum_predictions ])
ensemble_predictions
print('ensemble accuracy on validation set is ', str(accuracy_score(y_test, ensemble_predictions)))

sum_predictions

print('tensorflow accuracy on validation set is ', str(accuracy_score(y_test, all_predictions['tf'])))

#Predict on the "real" test set, which is df_test2, true results are not know for df_test2
predictions_ensemble_final={}

if 'Transported' in vars_to_use_training:
    vars_to_use_training.remove('Transported')

df_test2 = df_test2[vars_to_use_training]

#predict for algorithms
for key, classifier in classifiers.items():
    clf = classifier
    clf.fit(X_train, y_train)
    
    clf_predictions = clf.predict(df_test2)
    predictions_ensemble_final[key] = clf_predictions

#predict for tensorflow
y_predictions_tf = model.predict(df_test2)
predictions_ensemble_final['tf'] = np.array([1 if prediction>0.5 else 0 for prediction in y_predictions_tf ])

#5 models will be used, this problem is of binary classification, so result of prediction is either 0 or 1
#if the sum of the 5 models prediction is 3 or higher it means that a majority of the models predicted that the passengers were transported

sub_predictions ={}
sub_predictions['RandomForest'] = predictions_ensemble_final['RandomForest']
sub_predictions['LGBM'] = predictions_ensemble_final['LGBM']
sub_predictions['tf'] = predictions_ensemble_final['tf']
sub_predictions['CatBoost'] = predictions_ensemble_final['CatBoost']
sub_predictions['SVC'] = predictions_ensemble_final['SVC']

sum_predictions = np.zeros(len(sub_predictions['tf']))
for pred in sub_predictions.values():
  for i, v in enumerate(pred):
    sum_predictions[i] += v


final_pred_ensemble = np.array([1 if s>=3 else 0 for s in sum_predictions ])
final_pred_ensemble

if use_probabilities:
  #Probabilities avg, instead of using 0 or 1 results, the probabilities of result
  #for each model are gathered and the mean is calculated
  y_predictions_tf = model.predict(X_test)
  all_predictions_prob = all_predictions
  all_predictions_prob['tf'] = y_predictions_tf

  best_predictions_prob ={}
  best_predictions_prob['RandomForest'] = all_predictions_prob['RandomForest']
  best_predictions_prob['tf'] = all_predictions_prob['tf']

  soma = np.zeros(len(best_predictions_prob['tf']))
  for pred in best_predictions_prob.values():
    for i, v in enumerate(pred):
      soma[i] += v

  number_models = len(best_predictions_prob)
  pred_prob = [x / number_models for x in soma]
  print(pred_prob)

  ensemble_predictions_prob = np.array([1 if s>=0.5 else 0 for s in pred_prob])
  ensemble_predictions_prob
  print(accuracy_score(y_test, ensemble_predictions_prob))

len(best_predictions)

"""# Testing"""

if 'Transported' in vars_to_use_training:
    vars_to_use_training.remove('Transported')
df_test2 = df_test2[vars_to_use_training]

y_predictions = model.predict(df_test2)
print(df_test2.shape)

if use_probabilities:
  predictions_for_submission = ensemble_predictions_prob
else:
  predictions_for_submission = final_pred_ensemble

var_score = ['True' if prediction>0.5 else 'False' for prediction in predictions_for_submission]
print(var_score)

score_df = pd.DataFrame(var_score)
score_df.to_csv('score_df.csv')

score_df

"""# Export"""

df_export = pd.DataFrame({"PassengerId":df_test_original["PassengerId"], "Transported":var_score})
df_export.to_csv('results.csv', index=False)